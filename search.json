[{"title":"Dormitry","url":"/2021/01/24/Dormitry/","content":"来看看我的大学寝室吧~\n\n都是漂亮的小姐姐\n林晨曦（班里唯二女孩纸 相依为命\n叶涵    （yea~\n杨艺茜（又名唐怀瑟\n亲爱的小羽羽（文艺少女\n\n\n\n","categories":["HDU's Life"],"tags":["Daily"]},{"title":"A scene to remember","url":"/2021/08/03/Imitation/","content":"Good afternoon, ladies and gentlemen. Today I would like to begin with a story.\nThere was once a physical therapist who traveled all the way from America to Africa to do a census about mountain gorillas. These gorillas are a main attraction to tourists from all over the world; this put them severely under threat of poaching and being put into the zoo. She went there out of curiosity, but what she saw strengthened her determination to devote her whole life to fighting for those beautiful creatures. She witnessed a scene, a scene taking us to a place we never imaged we’ve ever been, where in the very depth of the African rainforest, surrounded by trees, flowers and butterflies, the mother gorillas cuddled their babies. Yes, that’s a memorable scene in one of my favorite movies, called Gorillas in the Mist, based on a true story of Mrs. Dian Fossey, who spent most of her lifetime in Rwanda to protect the ecoenvironment there until the very end of her life. To me, the movie not only presents an unforgettable scene but also acts as a timeless reminder that we should not develop the tourist industry at the cost of our ecoenvironment.\nToday, we live in a world of prosperity but still threatened by so many new problems. On the one hand, tourism, as one of the most promising industries in the 21st century, provides people with the great opportunity to see everything there is to see and to go any place there is to go. It has become a lifestyle for some people, and has turned out to be the driving force in GDP growth. It has the magic to turn a backward town into a wonderland of prosperity. But on the other hand, many problems can occur—natural scenes aren’t natural anymore. Deforestation to heat lodges is devastating Nepal. \n","categories":["HDU's Learning"],"tags":["English speech"]},{"title":"Life shines on life","url":"/2021/08/02/Life-shines-on-life/","content":"“Yeah we all shine on, like the moon, and the stars, and the sun.”   ——John Lennon\n\n\n\nself-introHello, sir or madam. I’m Qi Yingying, a freshman learning computer science and technology in Hangzhou Dianzi University. In school, I’m a down-to-earth student. I believe in the words that “no pains, no gains”. Speaking of my hobbies, I’m a music buff and also love sporting. Running along Qianjiang River, immersing my ears into sweet songs and filling my eyes with the marvelous light of sun shine really keep me spellbound. \nLet’s move into the topic speech.\nTopic speechOver 85 years ago, The Red Army fears not the trials of the Long March, holding light ten thousand crags and torrents. The Five Ridges wind like gentle ripples, and the majestic Wumeng roll by. Oprah Winfrey said, “let your light shine. Shine within you so that it can shine on someone else.” yes, they still shines on our lives with their persistence and bravery. \nIn 2020, during the pandemic, retrogrades put on gloves and protective suits, devoted themselves into the fight against the virus. Their selflessness and love warmed our hearts in that cold winter.\nYeah we all shall shine on and we all can shine on, like the moon, and the stars, and the sun. Shining on life isn’t a tough task. Just by helping people in need and being grateful for the people who once helped us. Smile to a passer-by, pick up a rubbish on the road, give a hug to a crying man,  and say thank you to people who once support us. If only we are willing, we can shine our lives on lives.\nWe can shine in many ways. Nhat Hanh said, “Awareness is like the sun. When it shines on things, they are transformed.” Michael Jackson appeals people to love, heal and educate children. Let’s all do our duties on shining on lives.\n\n（写这些不想写的东西 已经要写吐了……\n果然 对于喜欢的东西 过犹不及……\n当初一腔热血，现在头都要没了……\n冲！！\n","categories":["HDU's Learning"],"tags":["English speech"]},{"title":"OpenCV (创建新项目)","url":"/2021/07/26/OPENCV1-0/","content":"开始学习啦~~\n\nStep 1Go to https://github.com/opencv/opencv and download the Latest Release.\nStep 2Add bin folder to the Environment Variables path.（编辑系统环境变量）\nD:\\opencv\\build\\x64\\vc15\\bin\nStep 3Create a New Visual Studio project C++ console.\nSet the platform target to x64.\nStep 4Add Directories by going to Project-Properties-Configuration Properties. （项目 - 属性 -  配置属性）\nVC++目录 - 包含目录 &amp;&amp; 库目录\nAdd build directories D:\\opencv\\build\\include \nAdd Library directories D:\\opencv\\build\\x64\\vc15\\lib \n链接器 - 输入\nAdd linker input opencv_world450d.lib \n不同版本不一样，数字后面有d的与debug有关，没有d的与release有关。\n具体操作可参照 https://www.bilibili.com/video/BV11A411T7rL?t=2\n视频相关资源下载 https://www.murtazahassan.com/courses/opencv-cpp-course/\n添加自己写的头文件项目 -&gt; 属性 -&gt; C/C++/常规/附加包含目录 -&gt; 把.h那一级文件放进去即可\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"OpenCV (相机标定)","url":"/2021/07/31/OPENCV1-3/","content":"“The moment you doubt whether you can fly, you cease for ever to be able to do it.” —— Peter Pan\n\nPr相机标定相机参数的估计过程称为相机标定。这意味着我们拥有关于相机的所有信息（参数或系数），这些信息用于确定真实世界中的) 3D 点与其在该标定相机捕获的图像中的相应 2D 投影（像素）之间的精确关系。通常这意味着恢复两种参数。\n内部参数: 例如透镜的焦距、光学中心和径向畸变系数。外部参数: 指相机相对于某些世界坐标系的方位(旋转和平移)。\n二维点与三维点的转化\n\\pmb{s}\n\\begin{vmatrix} \n\\pmb{x} \\\\ \\pmb{y} \\\\ \\pmb{1}\n\\end{vmatrix}\n = \n\\begin{vmatrix} \n\\pmb{f} & \\pmb{0} & \\pmb{0} & \\pmb{0} \\\\ \\pmb{0} & \\pmb{f} & \\pmb{0} & \\pmb{0} \\\\ \\pmb{0} & \\pmb{0} & \\pmb{1} & \\pmb{0}\n\\end{vmatrix}\n\n\\begin{vmatrix} \n\\pmb{X} \\\\ \\pmb{Y} \\\\ \\pmb{Z} \\\\ \\pmb{1}\n\\end{vmatrix}四种坐标系\n1、图像像素坐标系：表示场景中三维点在图像平面上的投影，其坐标原点在CCD图像平面的左上角，u轴平行于CCD平面水平向右，v轴垂直于u轴向下，坐标使用（u,v）来表示。注：这里的（u,v）表示的是该像素在数组中的列数和行数\n2、图像物理坐标系：其坐标原点在CCD图像平面的中心，x,y轴分别平行于图像像素坐标系的坐标轴，坐标用(x,y)表示。\n3、相机坐标系：以相机的光心为坐标系原点，X,Y轴平行于图像坐标系的X,Y轴，相机的光轴为Z轴，坐标系满足右手法则。注：这里所指的相机的光心可以简单的理解为相机透镜的几何中心\n4、世界坐标系：也称为绝对坐标系，用于表示场景点的绝对坐标\n世界参考系和相机参考系的转换https://blog.csdn.net/jiangxing11/article/details/106478020\nhttps://blog.csdn.net/qq_15029743/article/details/90215104\n投影方程\n\\pmb{s}\n\\begin{vmatrix} \n\\pmb{x} \\\\ \\pmb{y} \\\\ \\pmb{1}\n\\end{vmatrix}\n = \n\\begin{vmatrix} \n\\pmb{f_x} & \\pmb{0} & \\pmb{u_0} \\\\ \\pmb{0} & \\pmb{f_y} & \\pmb{v_0} \\\\ \\pmb{0} & \\pmb{0} & \\pmb{1}\n\\end{vmatrix}\n\n\\begin{vmatrix} \n\\pmb{r_1} & \\pmb{r_2} & \\pmb{r_3} & \\pmb{t_1} \\\\ \\pmb{r_4} & \\pmb{r_5} & \\pmb{r_6} & \\pmb{t_2} \\\\ \\pmb{r_7} & \\pmb{r_8} & \\pmb{r_9} & \\pmb{t_3}\n\\end{vmatrix}\n\n\\begin{vmatrix} \n\\pmb{X} \\\\ \\pmb{Y} \\\\ \\pmb{Z} \\\\ \\pmb{1}\n\\end{vmatrix}基于OpenCV的相机标定目标标定过程的目标是使用一组已知的三维点 $(X_w, Y_w, A_w)$ 及其对应的图像坐标 $(u, v)$\n找到 3×3矩阵 K , 3×3旋转矩阵 R , 3×1平移向量 T 。当我们得到相机的内部和外部参数值时，相机就被称为标定相机。\n总之，相机标定算法具有以下输入和输出：\n输入：具有已知二维图像坐标和三维世界坐标的点的图像集合。输出：3×3相机内参矩阵，每幅图像的旋转和平移。注意OpenCV中，相机内部矩阵不包含倾斜参数。所以矩阵的形式是：\n\n\\begin{vmatrix} \n\\pmb{f_x} & \\pmb{0} & \\pmb{c_x} \\\\ \\pmb{0} & \\pmb{f_y} & \\pmb{c_y} \\\\ \\pmb{0} & \\pmb{0} & \\pmb{1}\n\\end{vmatrix}方法\n校正：当我们完全控制成像过程时，执行校准的最佳方法是从不同的视角捕获一个物体或已知尺寸模式的多个图像。我们将在这篇文章中学习的基于棋盘的方法属于这一类。我们也可以使用已知尺寸的圆形图案，而不是棋盘格图案。\n几何线索：有时我们在场景中有其他的几何线索，如直线和消失点，可以用来标定。\n基于深度学习的：当我们对成像设置的控制非常小（例如，我们有场景的单个图像）时，仍然可以使用基于深度学习的方法获取相机的校准信息。\n\n步骤\n使用棋盘格模式定义真实世界坐标;\n从不同的角度捕获棋盘的多个图像;\n查找棋盘的2D坐标;\n校准相机\n\n实现OpenCV 推荐使用国际象棋棋盘的图案生成用于标定的三维场景点的集合。\n\n这个图案在每个方块的角点位置创建场景点；由于图案是平面的，可以假设棋盘位于 Z = 0,  X,Y 的坐标轴与网格对其的位置 。这样，标定时就只需从不同的视角拍摄棋盘图案。\n\n检测角点\n可以用OpenCV自带的函数自动检测棋盘图案中的角点。只需要提供一幅图像和棋盘尺寸（水平和垂直方向内部角点的数量），函数会返回图像中所有棋盘角点的位置，若无法找到图案，函数返回false。\n//输出图像角点的向量vector&lt;Point2f&gt; imageCorners;//棋盘内部角点的数量Size boardSize(7, 5);//获得棋盘角点bool foound = findChessboardCorners(image, boardSize, omageCorners);//image 棋盘图案\n棋盘内部角点数，如图\n\n画角点\n画出角点并用线条连接起来，连接次序即在想两种存储的次序。\ndrawChessboardCorners(image, boardSize, imageCorners, found);\n\n指定三维点\n自由选择单位，如厘米或英寸等。\n为了方便起见我们将方块的边长定位单位，这样可令点的坐标如 （0，0，0）（6，4，0） 这样便于表示。\n（假设棋盘纵深坐标为 z = 0 ）\n为了得到更多的点，需要从不同的视角对同一个标定图案拍摄更多的照片。\nOpenCV的标定函数假定由标定图案确定坐标系，并计算相机相对于坐标系的旋转量和平移量。\n把标定过程封装在 CameraCailbrator 中，\nclass CameraCailbrator&#123;    //每个向量的元素也是一个向量 表示一个视角的点集    //输入点 世界坐标系（每个正方形为一个单位）    vector&lt;vector&lt;Point3f&gt; &gt; objectPoints;    //点在图像中的位置（以像素为单位）    vector&lt;vector&lt;Point2f&gt; &gt; imagePoints;    //输出矩阵    Mat cameraMatrix;    Mat distCoeffs;    //指定标定方式的标志    int fllag;&#125;\n\n类库\n#ifndef CAMERACALIBRATOR_H#define CAMERACALIBRATOR_H#include &lt;vector&gt;#include &lt;iostream&gt;#include &lt;opencv2/core.hpp&gt;#include &quot;opencv2/imgproc.hpp&quot;#include &quot;opencv2/calib3d.hpp&quot;#include &lt;opencv2/highgui.hpp&gt;class CameraCalibrator &#123;    // 输入点：      // 世界坐标系中的点     //（每个正方形为一个单位）     std::vector&lt;std::vector&lt;cv::Point3f&gt; &gt; objectPoints;    // 点在图像中的位置（以像素为单位）     std::vector&lt;std::vector&lt;cv::Point2f&gt; &gt; imagePoints;    // 输出矩阵     cv::Mat cameraMatrix;    cv::Mat distCoeffs;    // 指定标定方式的标志    int flag;    // used in image undistortion     cv::Mat map1,map2;     bool mustInitUndistort;  public:    CameraCalibrator() : flag(0), mustInitUndistort(true) &#123;&#125;    // Open the chessboard images and extract corner points    int addChessboardPoints(const std::vector&lt;std::string&gt;&amp; filelist, cv::Size &amp; boardSize, std::string windowName=&quot;&quot;);    // Add scene points and corresponding image points    void addPoints(const std::vector&lt;cv::Point2f&gt;&amp; imageCorners, const std::vector&lt;cv::Point3f&gt;&amp; objectCorners);    // Calibrate the camera    double calibrate(const cv::Size imageSize);    // Set the calibration flag    void setCalibrationFlag(bool radial8CoeffEnabled=false, bool tangentialParamEnabled=false);    // Remove distortion in an image (after calibration)    cv::Mat remap(const cv::Mat &amp;image, cv::Size &amp;outputSize = cv::Size(-1, -1));    // Getters    cv::Mat getCameraMatrix() &#123; return cameraMatrix; &#125;    cv::Mat getDistCoeffs()   &#123; return distCoeffs; &#125;&#125;;#endif // CAMERACALIBRATOR_H\n\n这里采用增加标定点的方法\nint CameraCailbrator::addChessboardPoints(const vector&lt;string&gt; &amp; filelist, Size &amp; boardSize)&#123; //文件名列表 标定面板大小        //棋盘上的角点    vector&lt;Point2f&gt; imageCorners;    vector&lt;Point3f&gt; objectCorners;        //场景中的三维点 在棋盘坐标系中，初始化期盼中的角点 角点的三维坐标(i, j, 0)    for(int i = 0; i &lt; boardSize.height; ++i)&#123;        for(int j = 0; j &lt; boardSize.width; ++j)&#123;            objectCorners.push_back(Point3f(i, j, 0.0f));        &#125;    &#125;        //图像中的二维点    Mat image; //存储棋盘图像    int successes = 0;    //处理所有视角    for(int i = 0; i &lt; filelist.size(); ++i)&#123;        image = imread(filelist[i], 0);                bool found = findChessboardCorners(image, boardSize, imageCorners);                //取得角点上的亚像素级精度        if(found)&#123;            cornerSubPix(image, imageCorners, Size(5, 5), Size(-1, -1), TermCriteria(TermCriteria::MAX_ITER + TermCriteria::EPS, 30, 0.1));            //棋盘完好            if(imageCorners.size() == boardSize.area())&#123;                //加入同一视角得到的图像和场景点                addPoints(imageCorners, objectCorners);                successes++;            &#125;        &#125;    &#125;    return successes; &#125;\n处理完足够数量的棋盘图像（一般10~20个）后，就可以开始计算标定参数了\n//标定相机 返回重投影误差double CameraCalibrator::calibrate(Size &amp;imageSize)&#123;    //输出旋转量和平移量     vector&lt;Mat&gt; rvecs, tvecs;        //开始标定    return calibrateCamera(objectPoints, // 三维点                           imagePoints,  // 图像点                           imageSize,    // 图像尺寸                           cameraMatrix, // 输出相机矩阵                           distCoffes,   // 输出畸变矩阵                           rvecs, tvecs, // Rs，Ts                           flag);        // 设置选项&#125;\n重投影误差\n函数 calibrateCamera 在得到标定结果之前进行了优化，以便找到合适的内部参数和外部参数，使图像点的预定位置（根据三维点的投影计算得到）和实际位置（图像中的位置）之间的距离达到最小。每个点在标定过程中都会产生这个距离，它们的累加和就是重投影误差。\n畸变模型\n径向畸变：超广角镜头产生的典型畸变\n// 去除图像中的畸变（标定后）Mat CameraCalibrator::remap(const Mat &amp;image)&#123;    Mat undistorted;        if(mustInitUndistort)&#123; // 每个标定过程只调用一次                initUndistortRectifyMap(cameraMatrix, // 计算得到的相机矩阵                                distCoeffs,   // 计算得到的畸变矩阵                                Mat(),        // 可选矫正项（无）                                Mat(),        // 生成无畸变的相机矩阵                                image.size(), // 无畸变图像的尺寸                                CV_32FC1,     // 输出图片的类型                                map1, map2);  // x和y映射功能                mustInitUndistort = false;    &#125;    // 应用映射功能    remap(image, undistorted, map1, map2, INTER_LINEAR); // 插值类型        return undistorted;&#125;\n实现原理回到本文最前面的投影方程。\n方程中连续使用了两个矩阵，把三维空间的点转换到二维空间。\n第一个矩阵，相机的内部参数，是一个3*3的矩阵。是函数 calibrateCamera 输出的矩阵之一。此外还有一个 calibrationMatrixValues 函数，它根据标定矩阵，显示地返回内部参数值。\n第二个矩阵，外部参数，内容是输入的点，以相机为坐标系中心。它由一个旋转向量（3*3）和一个平移向量（3*1）组成。旋转向量 $r_1, r_2, … r_9$ ，平移向量 $t_1, t_2, t_3$ 。\n参考https://blog.csdn.net/LuohenYJ/article/details/104697062\nhttps://blog.csdn.net/jiangxing11/article/details/106478020\nhttps://blog.csdn.net/qq_15029743/article/details/90215104\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"OpenCV (相机姿态还原)","url":"/2021/08/02/OPENCV1-4/","content":"在已知物体三维结构的情况下，如何计算出相机的姿态。\n\n三维姿态solvePnP官方解释：\nhttp://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#solvepnp\nbool solvePnP(InputArray objectPoints, InputArray imagePoints, InputArray cameraMatrix, InputArray distCoeffs, OutputArray rvec, OutputArray tvec,bool useExtrinsicGuess=false, int flags=ITERATIVE )\n注意，OpenCV 中还提供了 SolvePnPRansac 函数。它使用 RANSAC 算法求解 PnP 问题。这个函数能识别出错误的物体点/图像点对，并将其标记为异常数据。\nRANSAC（随机抽样一致性）\n用于匹配图像的算法\nhttps://zhuanlan.zhihu.com/p/45532306\nhttps://blog.csdn.net/weixin_42990464/article/details/119254747\ncvPOSIT官方解释：\nhttp://www.opencv.org.cn/index.php/Cv%E7%85%A7%E7%9B%B8%E6%9C%BA%E5%AE%9A%E6%A0%87%E5%92%8C%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA#POSIT\nvoid cvPOSIT( CvPOSITObject* posit_object, CvPoint2D32f* image_points, double focal_length, CvTermCriteria criteria, CvMatr32f rotation_matrix,  CvVect32f translation_vector )\n两种函数的同与异同：\n\n输入都是 3D 点集和对应的 2D 点集，其中 cvPOSIT 的 3D 点包含在 posit_object 结构中\n输出包括旋转矩阵和位移向量\n\n异：\n\nsolvePnP 调用的是 cvFindExtrinsicCameraParams2 通过已知的内参进行未知外参求解，是一个精确解；而 cvPOSIT 是用仿射投影模型近似透视投影模型下，不断迭代计算出来的估计值(在物体深度变化相对于物体到摄像机的距离比较大的时候,这种算法可能不收敛)\nsolvePnP 输出的 rvec 是旋转向量，可以通过 Rodrigues 转换成旋转矩阵，有需要可以再转到欧拉角\n\n实现公园里的长椅\n用标定的相机进行拍照 并 标注8个点\n测量长椅的物理尺寸\n椅座：242.5cm53.5cm\\9cm\n靠背：242.5cm24cm\\9cm\n两者相距 12cm\n\n推导八个点的三维坐标设椅座与靠背的交叉线的左侧顶点作为坐标系原点\nvector&lt;Point3f&gt; objectPoints;objectPoints.push_back(Point3f(0, 45, 0));objectPoints.push_back(Point3f(242.5, 45, 0));objectPoints.push_back(Point3f(242.5, 21, 0));objectPoints.push_back(Point3f(0, 32, 0));objectPoints.push_back(Point3f(0, 9, -9));objectPoints.push_back(Point3f(242.5, 9, -9));objectPoints.push_back(Point3f(242.5, 9, 44.5));objectPoints.push_back(Point3f(0, 9, 44.5));\n在二维成像平面中，写出这些点的坐标vector&lt;Point2f&gt; imagePoints;objectPoints.push_back(Point2f(136,113));objectPoints.push_back(Point2f(379,114));objectPoints.push_back(Point2f(379,150));objectPoints.push_back(Point2f(138,135));objectPoints.push_back(Point2f(143,146));objectPoints.push_back(Point2f(381,166));objectPoints.push_back(Point2f(345,194));objectPoints.push_back(Point2f(103,161));\n调用 solvePnP 函数，计算拍照时相机与这些点之间的相对位置此函数实际上是通过旋转和平移，把物体坐标转换到以相机为中心的坐标系上（焦点为坐标原点）\n注意，该函数得到的旋转量是一个三维容器。表示物体绕着一个单位向量（旋转轴）转了某个角度。（轴 + 角度，罗德里格旋转公式 ）。在 OpenCV 中，旋转角度对应着输出的旋转向量的值，该向量与旋转轴一致，所以，投影公式中使用 Rodrigues 函数来获取旋转三维矩阵。\nMat rvec, tvec;solvePnP(objectPoints, imagePoints, // 对应的三维点和二维点         cameraMatrix, cameraDistCoeffs, // 标定（相机内参 和 相机畸变）         rvec, tvec); // 输出// 转换成三维旋转矩阵Mat rotation;Rodrigues(rvec, rotation);\n检验使用 cv::viz 模块可以显示三维信 息\ncv::viz:: ...\n使用样例\ncv::viz::Viz3d visualizer(&quot;Viz window&quot;); // 创建窗口visualizer.setBackgroundColor(cv::viz::Color::white()); // 白色背景// 创建一个虚拟相机cv::viz::WCameraPosition cam(cMatrix, // 内部参数矩阵 类型为Matx33d（Matx&lt;double, 3, 3&gt;                            image, // 平面上显示的图像                            30.0, // 缩放因子                            cv::viz::Color::black());// 在环境中添加虚拟相机visualizer.showWidget(&quot;Camera&quot;, cam);// 用长方形表示虚拟的长椅cv::viz::Wcube plane1(Point3f(0.0, 45.0, 0.0),                     Point3f(242.5, 21.0, -9.0),                     true, // 显示线条框架                     cv::viz::Color::blue());cv::viz::Wcube plane2(Point3f(0.0, 9.0, -9.0),                     Point3f(242.5, 0.0, 44.5),                     true, // 显示线条框架                     cv::viz::Color::blue());// 把虚拟物体加入到环境中visualizer.showWidget(&quot;top&quot;, plane1);visualizer.showWidget(&quot;bottom&quot;, plane2);// 虚拟长椅也放在坐标原点，然后用cv::solvePnP函数计算出以相机为中心的位置，并把长椅移动到该位置。这个过程在setWidgetPose方法中完成。这里只是根据估算值进行了旋转和平移Mat rotation;// 将rotation转换成3*3的旋转矩阵Rodrigues(rvec, rotation);// 移动长椅Affine3d pose(rotation, tvec);visualizer.setWidgetPose(&quot;top&quot;, pose);visualizer.setWidgetPose(&quot;bottom&quot;, pose);// 循环显示while(waitKey(100) == -1 &amp;&amp; !visualizer.wasStopped())&#123;        visualizer.spinOnce(1, // 暂停1s                       true); // 重绘&#125;// 关闭可视化窗口 或 在OpenCV图像窗口上输入任意键就可以结束循环。在循环内部移动物体（用setWidgetPose），即可产生动画。\n相机姿态更新https://blog.csdn.net/aptx704610875/article/details/48915149\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"OpenCV (处理视频序列)","url":"/2021/08/04/OPENCV1-7/","content":"Video ~ ~\n\n读取int main()&#123;\tVideoCapture capture(&quot;bike.avi&quot;);\tif(!capture.isOpen()) return 1;\t\tdouble rate = capture.get(CV_CAP_PROP_FPS);\t\tbool stop(false);\tMat frame;\tnamedWindow(&quot;Extracted Frame&quot;);\t\tint delay = 1000/rate;\t\twhile(!stop)&#123;\t\t\t\tif(!capture.read(frame)) break;\t\t\t\timshow(&quot;Extracted Frame&quot;, frame);\t\t\t\tif(waitKey(delay) &gt; 0) stop = true; \t&#125;\t\tcapture.release();\treturn 0;&#125; \n详细见 https://evaqwq.github.io/2021/07/28/OPENCV1-1/\n处理视频帧#include &quot;videoprocessor.h&quot;void draw(const cv::Mat&amp; img, cv::Mat&amp; out) &#123;\timg.copyTo(out);\tcv::circle(out, cv::Point(100,100),5,cv::Scalar(255,0,0),2);&#125;// processing functionvoid canny(cv::Mat&amp; img, cv::Mat&amp; out) &#123;   // Convert to gray   if (img.channels()==3)      cv::cvtColor(img,out,cv::COLOR_BGR2GRAY);   // Compute Canny edges   cv::Canny(out,out,100,200);   // Invert the image   cv::threshold(out,out,128,255,cv::THRESH_BINARY_INV);&#125;int main()&#123;\t// Open the video file\tcv::VideoCapture capture(&quot;bike.avi&quot;);//\tcv::VideoCapture capture(&quot;http://www.laganiere.name/bike.avi&quot;);\t// check if video successfully opened\tif (!capture.isOpened())\t\treturn 1;\t// Get the frame rate\tdouble rate= capture.get(cv::CAP_PROP_FPS);\tstd::cout &lt;&lt; &quot;Frame rate: &quot; &lt;&lt; rate &lt;&lt; &quot;fps&quot; &lt;&lt; std::endl;\tbool stop(false);\tcv::Mat frame; // current video frame\tcv::namedWindow(&quot;Extracted Frame&quot;);\t// Delay between each frame\t// corresponds to video frame rate\tint delay= 1000/rate;\tlong long i=0;\tstd::string b=&quot;bike&quot;;\tstd::string ext=&quot;.bmp&quot;;\t// for all frames in video\twhile (!stop) &#123;\t\t// read next frame if any\t\tif (!capture.read(frame))\t\t\tbreak;\t\tcv::imshow(&quot;Extracted Frame&quot;,frame);\t\tstd::string name(b);        std::ostringstream ss; ss &lt;&lt; std::setfill(&#x27;0&#x27;) &lt;&lt; std::setw(3) &lt;&lt; i; name+= ss.str(); i++;\t\tname+=ext;\t\tstd::cout &lt;&lt; name &lt;&lt;std::endl;\t\t\t\tcv::Mat test;\t\t//      cv::resize(frame, test, cv::Size(), 0.2,0.2);\t\t//\t\tcv::imwrite(name, frame);        //\t\tcv::imwrite(name, test);\t\t// introduce a delay\t\t// or press key to stop\t\tif (cv::waitKey(delay)&gt;=0)\t\t\t\tstop= true;\t&#125;\t// Close the video file\tcapture.release();\tcv::waitKey();\t// Now using the VideoProcessor class\t// Create instance\tVideoProcessor processor;\t// Open video file\tprocessor.setInput(&quot;bike.avi&quot;);\t// Declare a window to display the video\tprocessor.displayInput(&quot;Input Video&quot;);\tprocessor.displayOutput(&quot;Output Video&quot;);\t// Play the video at the original frame rate\tprocessor.setDelay(1000./processor.getFrameRate());\t// Set the frame processor callback function\tprocessor.setFrameProcessor(canny);\t// output a video\tprocessor.setOutput(&quot;bikeCanny.avi&quot;,-1,15);\t// stop the process at this frame\tprocessor.stopAtFrameNo(51);\t// Start the process\tprocessor.run();\tcv::waitKey();\t\treturn 0;&#125;\nVideoProcessor.h\n// The frame processor interfaceclass FrameProcessor &#123;  public:\t// processing method\tvirtual void process(cv:: Mat &amp;input, cv:: Mat &amp;output)= 0;&#125;;class VideoProcessor &#123;  private:\t  // the OpenCV video capture object\t  cv::VideoCapture capture;\t  // the callback function to be called \t  // for the processing of each frame\t  void (*process)(cv::Mat&amp;, cv::Mat&amp;);\t  // the pointer to the class implementing \t  // the FrameProcessor interface\t  FrameProcessor *frameProcessor;\t  // a bool to determine if the \t  // process callback will be called\t  bool callIt;\t  // Input display window name\t  std::string windowNameInput;\t  // Output display window name\t  std::string windowNameOutput;\t  // delay between each frame processing\t  int delay;\t  // number of processed frames \t  long fnumber;\t  // stop at this frame number\t  long frameToStop;\t  // to stop the processing\t  bool stop;\t  // vector of image filename to be used as input\t  std::vector&lt;std::string&gt; images; \t  // image vector iterator\t  std::vector&lt;std::string&gt;::const_iterator itImg;\t  // the OpenCV video writer object\t  cv::VideoWriter writer;\t  // output filename\t  std::string outputFile;\t  // current index for output images\t  int currentIndex;\t  // number of digits in output image filename\t  int digits;\t  // extension of output images\t  std::string extension;\t  // to get the next frame \t  // could be: video file; camera; vector of images\t  bool readNextFrame(cv::Mat&amp; frame);\t  // to write the output frame \t  // could be: video file or images\t  void writeNextFrame(cv::Mat&amp; frame);  public:\t  // Constructor setting the default values\t  VideoProcessor() : callIt(false), delay(-1), \t\t  fnumber(0), stop(false), digits(0), frameToStop(-1), \t      process(0), frameProcessor(0) &#123;&#125;\t  // set the name of the video file\t  bool setInput(std::string filename)\t  // set the camera ID\t  bool setInput(int id)\t  // set the vector of input images\t  bool setInput(const std::vector&lt;std::string&gt;&amp; imgs)\t  // set the output video file\t  // by default the same parameters than input video will be used\t  bool setOutput(const std::string &amp;filename, int codec=0, double framerate=0.0, bool isColor=true);\t  // set the output as a series of image files\t  // extension must be &quot;.jpg&quot;, &quot;.bmp&quot; ...\t  bool setOutput(const std::string &amp;filename, // filename prefix\t\t  const std::string &amp;ext, // image file extension \t\t  int numberOfDigits=3,   // number of digits\t\t  int startIndex=0) // start index\t  // set the callback function that will be called for each frame\t  void setFrameProcessor(void (*frameProcessingCallback)(cv::Mat&amp;, cv::Mat&amp;));\t  // set the instance of the class that implements the FrameProcessor interface\t  void setFrameProcessor(FrameProcessor* frameProcessorPtr);\t  // stop streaming at this frame number\t  void stopAtFrameNo(long frame);\t  // process callback to be called\t  void callProcess();\t  // do not call process callback\t  void dontCallProcess();\t  // to display the input frames\t  void displayInput(std::string wn);\t  // to display the processed frames\t  void displayOutput(std::string wn);\t  // do not display the processed frames\t  void dontDisplay();\t  // set a delay between each frame\t  // 0 means wait at each frame\t  // negative means no delay\t  void setDelay(int d);\t  // a count is kept of the processed frames\t  long getNumberOfProcessedFrames();\t  // return the size of the video frame\t  cv::Size getFrameSize();\t  // return the frame number of the next frame\t  long getFrameNumber();\t  // return the position in ms\t  double getPositionMS();\t  // return the frame rate\t  double getFrameRate();\t  // return the number of frames in video\t  long getTotalFrameCount();\t  // get the codec of input video\t  int getCodec(char codec[4]);\t  \t  // go to this frame number\t  bool setFrameNumber(long pos);\t  // go to this position\t  bool setPositionMS(double pos);\t  // go to this position expressed in fraction of total film length\t  bool setRelativePosition(double pos);\t  // Stop the processing\t  void stopIt();\t  // Is the process stopped?\t  bool isStopped();\t  // Is a capture device opened?\t  bool isOpened();\t  \t  // to grab (and process) the frames of the sequence\t  void run();&#125;;\n提取前景物BGFGSegmentor.h\nclass BGFGSegmentor : public FrameProcessor &#123;\t\tcv::Mat gray;\t\t\t// current gray-level image\tcv::Mat background;\t\t// accumulated background\tcv::Mat backImage;\t\t// current background image\tcv::Mat foreground;\t\t// foreground image\tdouble learningRate;    // learning rate in background accumulation\tint threshold;\t\t\t// threshold for foreground extraction  public:\tBGFGSegmentor() : threshold(10), learningRate(0.01) &#123;&#125;\t// Set the threshold used to declare a foreground\tvoid setThreshold(int t) &#123;\t\tthreshold= t;\t&#125;\t// Set the learning rate\tvoid setLearningRate(double r) &#123;\t\tlearningRate= r;\t&#125;\t// processing method\tvoid process(cv:: Mat &amp;frame, cv:: Mat &amp;output) &#123;\t\t// convert to gray-level image\t\tcv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY); \t\t// initialize background to 1st frame\t\tif (background.empty())\t\t\tgray.convertTo(background, CV_32F);\t\t\t\t// convert background to 8U\t\tbackground.convertTo(backImage,CV_8U);\t\t// compute difference between current image and background\t\tcv::absdiff(backImage,gray,foreground);\t\t// apply threshold to foreground image\t\tcv::threshold(foreground,output,threshold,255,cv::THRESH_BINARY_INV);\t\t// accumulate background\t\tcv::accumulateWeighted(gray, background, \t\t\t                   // alpha*gray + (1-alpha)*background\t\t\t                   learningRate,  // alpha \t\t\t\t\t\t\t   output);       // mask\t&#125;&#125;;#endif\nint main()&#123;\t// Open the video file    cv::VideoCapture capture(&quot;bike.avi&quot;);\t// check if video successfully opened\tif (!capture.isOpened())\t\treturn 0;\t// current video frame\tcv::Mat frame; \t// foreground binary image\tcv::Mat foreground;\t// background image\tcv::Mat background;\tcv::namedWindow(&quot;Extracted Foreground&quot;);\t// The Mixture of Gaussian object\t// used with all default parameters\tcv::Ptr&lt;cv::BackgroundSubtractor&gt; ptrMOG = cv::bgsegm::createBackgroundSubtractorMOG();\t\tbool stop(false);\t// for all frames in video\twhile (!stop) &#123;\t\t// read next frame if any\t\tif (!capture.read(frame))\t\t\tbreak;\t\t// update the background\t\t// and return the foreground\t\tptrMOG-&gt;apply(frame,foreground,0.01);\t\t// Complement the image\t\tcv::threshold(foreground,foreground,128,255,cv::THRESH_BINARY_INV);\t\t// show foreground and background\t\tcv::imshow(&quot;Extracted Foreground&quot;,foreground);\t\t// introduce a delay\t\t// or press key to stop\t\tif (cv::waitKey(10)&gt;=0)\t\t\t\tstop= true;\t&#125;\tcv::waitKey();\t// Create video procesor instance\tVideoProcessor processor;\t// Create background/foreground segmentor \tBGFGSegmentor segmentor;\tsegmentor.setThreshold(25);\t// Open video file\tprocessor.setInput(&quot;bike.avi&quot;);\t// set frame processor\tprocessor.setFrameProcessor(&amp;segmentor);\t// Declare a window to display the video\tprocessor.displayOutput(&quot;Extracted Foreground&quot;);\t// Play the video at the original frame rate\tprocessor.setDelay(1000./processor.getFrameRate());\t// Start the process\tprocessor.run();\tcv::waitKey();&#125; \n将得到一些二值前景图像\n\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"OpenCV (用标定相机实现三维重建)","url":"/2021/08/02/OPENCV1-5/","content":"  利用不同视角下图像点之间的关系，计算出三维信息。\n\n学些啥？\n利用不同视角下图像点之间的关系，计算出三维信息\n\n新的数学实体 —— 本质矩阵\n\n三角剖分概念\n\n\n\n实现匹配两个视图的特征点可利用 SIFT检测器 或 描述子\nSIFT &amp; SURFSURF（加速稳健特征） 算法是 SIFT（尺度不变特征转换） 算法的加速版\nSURF 特征检测属于 opencv_contrib 库，在编译时包含了附加模块才能使用。\n// 创建SURF特征检测器对象Ptr&lt;xfeatures2d::SurfFeatureDetector&gt; ptrSURF = xfeatures2d::SurfFeatureDetector::create((2000.0));// 检测关键点ptrSURF -&gt; detect(image, keypoints);\nSIFT\n// 创建SIFT特征检测器对象Ptr&lt;xfeatures2d::SiftFeatureDetector&gt; ptrSIFT = xfeatures2d::SiftFeatureDetector::create((2000.0));// 检测关键点ptrSIFT -&gt; detect(image, keypoints);\n画关键点\ndrawKeypoints(image,                                  // 原始图像              keypoints,                              // 关键点的向量              featureImage,                           // 结果图像              Scalar(255, 255, 255),                  // 点的颜色              DrawMatchesFlags::DRAW_RICH_KEYPOINTS); // 显示相关的因子尺度\n包含被检测特征的结果图像\n\n用不同的尺度对同一物体拍摄一张照片，特征检测的结果图像\n\n DrawMatchesFlags::DRAW_RICH_KEYPOINTS 标志得到了关键点的圆，并且圆的尺寸与每个特征计算得到的尺度成正比。为了使特征具有旋转不变性，SURF还让每个特征关联了一个方向，由每个圆的辐射线表示。\n特征描述子// 定义关键点的容器vector&lt;KeyPoint&gt; keypoints1;vector&lt;KeyPoint&gt; keypoints2;// 定义特征检测器Ptr&lt;Feature2D&gt; ptrFeature2D = xfeatures2d::SURF::create(2000.0);// 检测关键点ptrFeature2D -&gt; detect(image1, keypoints1);ptrFeature2D -&gt; detect(image2, keypoints2);// 提取描述子Mat descriptors1;Mat descriptors2;ptrFeature2D -&gt; compute(image1, keypoints1, descriptors1);ptrFeature2D -&gt; compute(image2, keypoints2, descriptors2);// 构造匹配器BFMatcher matcher(NORM_L2);// 匹配两幅图像的描述子vector&lt;DMatch&gt; matches;matcher.match(descriptors1, descriptors2, matches);\nFeature2D 类有一个很实用的函数，可在检测兴趣点的同时计算它们的描述子，调用方法如下\nptrFeature2D -&gt; detectAndCompute(image, noArray(), keypoints, descriptors);\n二值描述子ORB\nORB 的用法与 SURF 、SIFT 没有什么区别\n// 定义关键点的容器 和 描述子vector&lt;KeyPoint&gt; keypoints1;vector&lt;KeyPoint&gt; keypoints2;Mat descriptors1;Mat descriptors2;// 定义特征检测器 / 描述子Ptr&lt;Feature2D&gt; feature = ORB::create(60);// 检测 并 描述 关键点feature -&gt; detectAndCompute(image1, noArray(), keypoints1, descriptors1);feature -&gt; detectAndCompute(image2, noArray(), keypoints2, descriptors2);// 构造匹配器BFMatcher matcher(NORM_HAMMING); // 二值描述子一律用HAMMING规范// 匹配两幅图像的描述子vector&lt;DMatch&gt; matches;matcher.match(descriptors1, descriptors2, matches);\n找出本质矩阵https://www.cnblogs.com/yuanlibin/p/9462180.html\nMat inliers;Mat essential findEssentialMat(points1, points2,                              Matrix, // 内部参数                              RANSAC,                              0.9, 1.0, // RANSAC方法                              inliers); // 提取到的内殿\n还原相机的相对姿态// 根据本质矩阵还原相机的相对姿态Mat rotation, trnslation;recoverPose(essential, // 本质矩阵            points1, points2, // 匹配的关键点            cameraMatrix, // 内部矩阵            rotation, translation, // 计算的移动值            inliers); // 内点匹配项\n计算三角剖分vector&lt;Vec3d&gt; points3D;triangulate(projection1, projection2, points1u, points2u, points3D);\n\n完整代码// 定义关键点的容器 和 描述子vector&lt;KeyPoint&gt; keypoints1;vector&lt;KeyPoint&gt; keypoints2;Mat descriptors1;Mat descriptors2;// 定义SIFT特征检测器Ptr&lt;Feature2D&gt; ptrFeature2D = xfeatures2d::SIFT::create(500);// 检测 并 描述 关键点feature -&gt; detectAndCompute(image1, noArray(), keypoints1, descriptors1);feature -&gt; detectAndCompute(image2, noArray(), keypoints2, descriptors2);// 构造匹配器BFMatcher matcher(NORM_L2, true); // 交叉检查标志 true// 匹配两幅图像的描述子vector&lt;DMatch&gt; matches;matcher.match(descriptors1, descriptors2, matches);// 将关键点转换成 Point2f 类型vector&lt;Point2f&gt; points1, points2;for(vector&lt;DMatch&gt;::const_iterator it = matches.begin(); it!= matches.end(); ++it)&#123;    // 获取 左 右 侧关键点的位置    float x = keypoints1[it -&gt; queryIdx].pt.x;    float y = keypoints1[it -&gt; queryIdx].pt.y;    points1.push_back(Point2f(x, y));        float x = keypoints2[it -&gt; queryIdx].pt.x;    float y = keypoints2[it -&gt; queryIdx].pt.y;    points2.push_back(Point2f(x, y));&#125;// 找出image1，image2之间的本质矩阵Mat inliers;Mat essential findEssentialMat(points1, points2,                              Matrix, // 内部参数                              RANSAC,                              0.9, 1.0, // RANSAC方法                              inliers); // 提取到的内殿// 根据本质矩阵还原相机的相对姿态Mat rotation, trnslation;recoverPose(essential, // 本质矩阵            points1, points2, // 匹配的关键点            cameraMatrix, // 内部矩阵            rotation, translation, // 计算的移动值            inliers); // 内点匹配项// 根据旋转量R和平移量T构建投影矩阵Mat projection2(3, 4, CV_64F); // 3*4rotation.copyTo(projection2(Rect(0, 0, 3, 3)));translation.copyTo(projection2.colRange(3, 4));// 构建通用投影矩阵Mat projection1(3, 4, CV_64F, 0);Mat diag(Mat::eye(3, 3, CV_64F));diag.copyTo(projection1(Rect(0, 0, 3, 3)));// 用于存储内点vector&lt;Vec2d&gt; inlierPts1;vector&lt;Vec2d&gt; inlierPts2;// 创建输入内点的容器，用于三角剖分int j(0);for(int i = 0; i &lt; inliers.rows; ++i)&#123;    if(inliers.at&lt;uchar&gt;(i))&#123;    \tinlierPts1.push_back(Vec2d(points1[i].x, points1[i].y));        inlierPts2.push_back(Vec2d(points2[i].x, points2[i].y));    &#125;&#125;// 矫正并标准化图像点vector&lt;Vec2d&gt; points1u;undistortPoints(inlierPts1, points1u, cameraMatrix, cameraDistCoeffs);vector&lt;Vec2d&gt; points2u;undistortPoints(inlierPts2, points2u, cameraMatrix, cameraDistCoeffs);// 三角剖分vector&lt;Vec3d&gt; points3D;triangulate(projection1, projection2, points1u, points2u,)\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"OpenCV (用形态学运算变换图像)","url":"/2021/08/05/OPENCV1-9/","content":"数学形态学！？！？！\n\nrecommendation: https://blog.csdn.net/keen_zuxwang/article/details/72768092?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162806274916780264097167%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=162806274916780264097167&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-72768092.first_rank_v2_pc_rank_v29&amp;utm_term=morphologyex&amp;spm=1018.2226.3001.4187\n基本概念\n\n形态学滤波器通常用于二值图像。且在形态学中，我们习惯用高像素值（白）表示前景物体，低像素值（黑）表示背景物体，因此对图像做了反向处理。（反向处理后的图像称为原二值图像的 补码 ）\n\n形态学滤波器 —— 腐蚀和膨胀滤波器的作用范围是由结构元素定义的像素集。在某个像素上应用结构元素时，结构元素的锚点与该像素对齐，所有与结构元素相交的像素就包含在当前集合中。腐蚀 就是把当前像素替换成所定义像素集合中的最小像素值，膨胀 就是把当前像素替换成所定义像素集合中的最大像素值。\nint main()&#123;\t// Read input image\tcv::Mat image= cv::imread(&quot;binary.bmp&quot;);\tif (!image.data)\t\treturn 0;     // Display the image\tcv::namedWindow(&quot;Image&quot;);\tcv::imshow(&quot;Image&quot;,image);\t// Erode the image\t// with the default 3x3 structuring element (SE)\tcv::Mat eroded; // the destination image\tcv::erode(image,eroded,cv::Mat());    // Display the eroded image\tcv::namedWindow(&quot;Eroded Image&quot;);\tcv::imshow(&quot;Eroded Image&quot;,eroded);\t// Dilate the image\tcv::Mat dilated; // the destination image\tcv::dilate(image,dilated,cv::Mat());    // Display the dilated image\tcv::namedWindow(&quot;Dilated Image&quot;);\tcv::imshow(&quot;Dilated Image&quot;,dilated);\t    return 0;&#125;\nOpenCV 默认使用 3*3 正方形结构元素，将第三个元素指定为空矩阵。也可自定义正方形大小。\n// Erode the image with a larger SE// create a 7x7 mat with containing all 1scv::Mat element(7,7,CV_8U,cv::Scalar(1));// erode the image with that SEcv::erode(image,eroded,element);// Display the eroded imagecv::namedWindow(&quot;Eroded Image (7x7)&quot;);cv::imshow(&quot;Eroded Image (7x7)&quot;,eroded);\n除了改变结构元素的大小，还可以改变应用一个结构元素的次数。\n// Erode the image 3 times.cv::erode(image,eroded,cv::Mat(),cv::Point(-1,-1),3);// Display the eroded imagecv::namedWindow(&quot;Eroded Image (3 times)&quot;);cv::imshow(&quot;Eroded Image (3 times)&quot;,eroded);\nPoint(-1, -1) 表示原点是矩阵的中心点（默认），也可以定义在结构元素上的其他位置。\n形态学滤波器 —— 开启和闭合图像闭合 ：对图像先膨胀后腐蚀\n// Close the imagecv::Mat element5(5,5,CV_8U,cv::Scalar(1));cv::Mat closed;cv::morphologyEx(image,closed,    // input and output images                 cv::MORPH_CLOSE, // operator code\t             element5);       // structuring element// Display the closed imagecv::namedWindow(&quot;Closed Image&quot;);cv::imshow(&quot;Closed Image&quot;,closed);// explicit closing// 1. dilate original imagecv::Mat result;cv::dilate(image, result, element5);// 2. in-place erosion of the dilated imagecv::erode(result, result, element5);// Display the closed imagecv::namedWindow(&quot;Closed Image (2)&quot;);cv::imshow(&quot;Closed Image (2)&quot;, result);\n5*5 的结构元素\n开启 ：先腐蚀后膨胀\n// Open the imagecv::Mat opened;cv::morphologyEx(image,opened,cv::MORPH_OPEN,element5);// Display the opened imagecv::namedWindow(&quot;Opened Image&quot;);cv::imshow(&quot;Opened Image&quot;,opened);// explicit openningcv::Mat result;// 1. in-place erosion of the dilated imagecv::erode(result, result, element5);// 2. dilate original imagecv::dilate(image, result, element5);// Display the opened imagecv::namedWindow(&quot;Opened Image (2)&quot;);cv::imshow(&quot;Opened Image (2)&quot;, result);\n注：开启和闭合运算是 幂等 的，所以重复开启或者闭合是没有作用的。\n应用形态学运算 —— 灰度图像形态学梯度运算 可以提取出图像边缘。\n// Read input image (gray-level)image = cv::imread(&quot;boldt.jpg&quot;,0);if (!image.data)\treturn 0;// Get the gradient image using a 3x3 structuring elementcv::morphologyEx(image, result, cv::MORPH_GRADIENT, cv::Mat());// Display the morphological edge imagecv::namedWindow(&quot;Edge Image&quot;);cv::imshow(&quot;Edge Image&quot;, 255 - result); // 反色处理，便于观察// Apply threshold to obtain a binary imageint threshold(80);cv::threshold(result, result, threshold, 255, cv::THRESH_BINARY);// Display the close/opened imagecv::namedWindow(&quot;Thresholded Edge Image&quot;);cv::imshow(&quot;Thresholded Edge Image&quot;, result);// Get the gradient image using a 3x3 structuring elementcv::morphologyEx(image, result, cv::MORPH_GRADIENT, cv::Mat());\n顶帽变换 可以从图像中提取出局部的小型前景物体\nimage = cv::imread(&quot;book.jpg&quot;, 0);if (!image.data)\treturn 0;// rotate the image for easier displaycv::transpose(image, image);cv::flip(image, image, 0);// Apply the black top-hat transform using a 7x7 structuring elementcv::Mat element7(7, 7, CV_8U, cv::Scalar(1));cv::morphologyEx(image, result, cv::MORPH_BLACKHAT, element7);// Display the top-hat imagecv::namedWindow(&quot;7x7 Black Top-hat Image&quot;);cv::imshow(&quot;7x7 Black Top-hat Image&quot;, 255-result);// Apply threshold to obtain a binary imagethreshold= 25;cv::threshold(result, result,\tthreshold, 255, cv::THRESH_BINARY);// Display the morphological edge imagecv::namedWindow(&quot;Thresholded Black Top-hat&quot;);cv::imshow(&quot;Thresholded Black Top-hat&quot;, 255 - result);// Apply the black top-hat transform using a 7x7 structuring elementcv::morphologyEx(image, result, cv::MORPH_CLOSE, element7);// Display the top-hat imagecv::namedWindow(&quot;7x7 Closed Image&quot;);cv::imshow(&quot;7x7 Closed Image&quot;, 255 - result);\n分水岭算法 —— 图像分割封装 WatershedSegmenter 类\n#if !defined WATERSHS#define WATERSHS#include &lt;opencv2/core.hpp&gt;#include &lt;opencv2/imgproc.hpp&gt;class WatershedSegmenter &#123;  private:\t  cv::Mat markers;  public:\t  void setMarkers(const cv::Mat&amp; markerImage) &#123;\t\t// Convert to image of ints\t\tmarkerImage.convertTo(markers,CV_32S);\t  &#125;\t  cv::Mat process(const cv::Mat &amp;image) &#123;\t\t// Apply watershed\t\tcv::watershed(image,markers);\t\treturn markers;\t  &#125;\t  // Return result in the form of an image\t  cv::Mat getSegmentation() &#123;\t\t  \t\tcv::Mat tmp;\t\t// all segment with label higher than 255\t\t// will be assigned value 255\t\tmarkers.convertTo(tmp,CV_8U);\t\treturn tmp;\t  &#125;\t  // Return watershed in the form of an image\t  cv::Mat getWatersheds() &#123;\t\t\tcv::Mat tmp;\t\tmarkers.convertTo(tmp,CV_8U,255,255);\t\treturn tmp;\t  &#125;&#125;;#endif\n步骤读取\n// Read input imagecv::Mat image= cv::imread(&quot;group.jpg&quot;);if (!image.data)\treturn 0; // Display the imagecv::namedWindow(&quot;Original Image&quot;);cv::imshow(&quot;Original Image&quot;,image);\n获得二值图像\n// Get the binary mapcv::Mat binary;binary= cv::imread(&quot;binary.bmp&quot;,0);// Display the binary imagecv::namedWindow(&quot;Binary Image&quot;);cv::imshow(&quot;Binary Image&quot;,binary);\n做深度腐蚀运算，只保留明显属于前景物体的像素\n// Eliminate noise and smaller objectscv::Mat fg;cv::erode(binary,fg,cv::Mat(),cv::Point(-1,-1),4);// Display the foreground imagecv::namedWindow(&quot;Foreground Image&quot;);cv::imshow(&quot;Foreground Image&quot;,fg);\n做大幅度膨胀，来选中一些背景像素\n// Identify image pixels without objectscv::Mat bg;cv::dilate(binary,bg,cv::Mat(),cv::Point(-1,-1),4);cv::threshold(bg,bg,1,128,cv::THRESH_BINARY_INV);// Display the background imagecv::namedWindow(&quot;Background Image&quot;);cv::imshow(&quot;Background Image&quot;,bg);\n合并这两幅图，得到标记图像\n// Show markers imagecv::Mat markers(binary.size(),CV_8U,cv::Scalar(0));markers= fg+bg;cv::namedWindow(&quot;Markers&quot;);cv::imshow(&quot;Markers&quot;,markers);\n创建分水岭分割类对象 并 进行分割\n// Create watershed segmentation objectWatershedSegmenter segmenter;// Set markers and processsegmenter.setMarkers(markers);segmenter.process(image);// Display segmentation resultcv::namedWindow(&quot;Segmentation&quot;);cv::imshow(&quot;Segmentation&quot;,segmenter.getSegmentation());\n以图像的方式返回分水岭\n// Display watershedscv::namedWindow(&quot;Watersheds&quot;);cv::imshow(&quot;Watersheds&quot;,segmenter.getWatersheds());\n另一种方法用户可以交互式地在场景中地物体和背景上绘制区域，以标注物体。\n如这幅图，在边缘位置（背景）与 中心位置（前景）分别标记\n\n// Open another imageimage= cv::imread(&quot;tower.jpg&quot;);// Identify background pixelscv::Mat imageMask(image.size(),CV_8U,cv::Scalar(0));cv::rectangle(imageMask,cv::Point(5,5),cv::Point(image.cols-5,image.rows-5),cv::Scalar(255),3);// Identify foreground pixels (in the middle of the image)cv::rectangle(imageMask,cv::Point(image.cols/2-10,image.rows/2-10),\t\t\t\t\t\t    cv::Point(image.cols/2+10,image.rows/2+10),cv::Scalar(1),10);// Set markers and processsegmenter.setMarkers(imageMask);segmenter.process(image);// Display the image with markerscv::rectangle(image,cv::Point(5,5),cv::Point(image.cols-5,image.rows-5),cv::Scalar(255,255,255),3);cv::rectangle(image,cv::Point(image.cols/2-10,image.rows/2-10),\t\t\t  cv::Point(image.cols/2+10,image.rows/2+10),cv::Scalar(1,1,1),10);cv::namedWindow(&quot;Image with marker&quot;);cv::imshow(&quot;Image with marker&quot;,image);// Display watershedscv::namedWindow(&quot;Watershed&quot;);cv::imshow(&quot;Watershed&quot;,segmenter.getWatersheds());\nMSER算法 —— 提取特征区域MSER 最大稳定外部区域\nint main()&#123;\t// Read input image\tcv::Mat image= cv::imread(&quot;building.jpg&quot;,0);\tif (!image.data)\t\treturn 0;     // Display the image\tcv::namedWindow(&quot;Image&quot;);\tcv::imshow(&quot;Image&quot;,image);\t\t// basic MSER detector\tcv::Ptr&lt;cv::MSER&gt; ptrMSER= cv::MSER::create(5,     // delta value for local minima detection\t\t                                        200,   // min acceptable area \t\t\t\t                                2000); // max acceptable area    // vector of point sets\tstd::vector&lt;std::vector&lt;cv::Point&gt; &gt; points;\t// vector of rectangles\tstd::vector&lt;cv::Rect&gt; rects;\t// detect MSER features\tptrMSER-&gt;detectRegions(image, points, rects);\tstd::cout &lt;&lt; points.size() &lt;&lt; &quot; MSERs detected&quot; &lt;&lt; std::endl;\n检测结果放在两个容器里。第一个是区域地容器，每个区域用组成它的像素点表示；第二个是矩形的容器，每个矩形包围一个区域。为了呈现效果，创建一个空白图像，在图像上用不同的颜色显示检测到的区域。\n// create white imagecv::Mat output(image.size(),CV_8UC3);output= cv::Scalar(255,255,255);// OpenCV random number generatorcv::RNG rng;// Display the MSERs in color areas// for each detected feature// reverse order to display the larger MSER first   for (std::vector&lt;std::vector&lt;cv::Point&gt; &gt;::reverse_iterator it= points.rbegin();\t\t   it!= points.rend(); ++it) &#123;\t// generate a random color\tcv::Vec3b c(rng.uniform(0,254),\t\t        rng.uniform(0,254),\t\t\t\trng.uniform(0,254));\tstd::cout &lt;&lt; &quot;MSER size= &quot; &lt;&lt; it-&gt;size() &lt;&lt; std::endl;\t// for each point in MSER set\tfor (std::vector&lt;cv::Point&gt;::iterator itPts= it-&gt;begin();\t\t                                  itPts!= it-&gt;end(); ++itPts) &#123;\t\t//do not overwrite MSER pixels\t\tif (output.at&lt;cv::Vec3b&gt;(*itPts)[0]==255) &#123;\t\t\toutput.at&lt;cv::Vec3b&gt;(*itPts)= c;\t\t&#125;\t&#125;&#125;cv::namedWindow(&quot;MSER point sets&quot;);cv::imshow(&quot;MSER point sets&quot;,output);cv::imwrite(&quot;mser.bmp&quot;, output);\n// Extract and display the rectangular MSERs\tstd::vector&lt;cv::Rect&gt;::iterator itr = rects.begin();\tstd::vector&lt;std::vector&lt;cv::Point&gt; &gt;::iterator itp = points.begin();\tfor (; itr != rects.end(); ++itr, ++itp) &#123;\t\t// ratio test\t\tif (static_cast&lt;double&gt;(itp-&gt;size())/itr-&gt;area() &gt; 0.6)\t\t\tcv::rectangle(image, *itr, cv::Scalar(255), 2);\t&#125;\t// Display the resulting image\tcv::namedWindow(&quot;Rectangular MSERs&quot;);\tcv::imshow(&quot;Rectangular MSERs&quot;, image);\n依据检测到的区域不能太细长（将封闭矩形旋转，计算高宽比），用封闭椭圆表示\n\t// Reload the input image\timage = cv::imread(&quot;building.jpg&quot;, 0);\tif (!image.data)\t\treturn 0;\t// Extract and display the elliptic MSERs\tfor (std::vector&lt;std::vector&lt;cv::Point&gt; &gt;::iterator it = points.begin();\t                                                    it != points.end(); ++it) &#123;\t\t// for each point in MSER set\t\tfor (std::vector&lt;cv::Point&gt;::iterator itPts = it-&gt;begin();\t\t                                      itPts != it-&gt;end(); ++itPts) &#123;\t\t\t// Extract bouding rectangles\t\t\tcv::RotatedRect rr = cv::minAreaRect(*it);            // check ellipse elongation\t\t\tif (rr.size.height / rr.size.height &gt; 0.6 || rr.size.height / rr.size.height &lt; 1.6)\t\t\t\tcv::ellipse(image, rr, cv::Scalar(255), 2);\t\t&#125;\t&#125;\t// Display the image\tcv::namedWindow(&quot;MSER ellipses&quot;);\tcv::imshow(&quot;MSER ellipses&quot;, image);\tcv::waitKey();&#125;\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"Speech_Information cocoon","url":"/2021/08/08/Speech-InformationCocoon/","content":"Tomorrow’s homework ~ ~\n","categories":["HDU's Learning"],"tags":["English speech"]},{"title":"OpenCV (跟踪运动目标)","url":"/2021/08/04/OPENCV1-8/","content":"Tracking ~ ~\n\n学这有啥用嘞对于手持摄像机拍摄的视频，可以用这种方法消除抖动或减小抖动幅度，时视频更加平稳\n运功估值还可用于视频编码，用以压缩视频，便于传输和存储\n跟踪特征点在最初的帧中检测特征点，在下一帧中跟踪这些特征点\nclass FeatureTracker : public FrameProcessor &#123;\t\tcv::Mat gray;\t\t\t// current gray-level image\tcv::Mat gray_prev;\t\t// previous gray-level image\tstd::vector&lt;cv::Point2f&gt; points[2]; // tracked features from 0-&gt;1\tstd::vector&lt;cv::Point2f&gt; initial;   // initial position of tracked points\tstd::vector&lt;cv::Point2f&gt; features;  // detected features\tint max_count;\t  // maximum number of features to detect\tdouble qlevel;    // quality level for feature detection\tdouble minDist;   // minimum distance between two feature points\tstd::vector&lt;uchar&gt; status; // status of tracked features    std::vector&lt;float&gt; err;    // error in tracking  public:\tFeatureTracker() : max_count(500), qlevel(0.01), minDist(10.) &#123;&#125;\t\t// processing method\tvoid process(cv:: Mat &amp;frame, cv:: Mat &amp;output) &#123;\t\t// convert to gray-level image\t\tcv::cvtColor(frame, gray, CV_BGR2GRAY); \t\tframe.copyTo(output);\t\t// 1. if new feature points must be added\t\tif(addNewPoints())\t\t&#123;\t\t\t// detect feature points\t\t\tdetectFeaturePoints();\t\t\t// add the detected features to the currently tracked features\t\t\tpoints[0].insert(points[0].end(),features.begin(),features.end());\t\t\tinitial.insert(initial.end(),features.begin(),features.end());\t\t&#125;\t\t\t\t// for first image of the sequence\t\tif(gray_prev.empty())           gray.copyTo(gray_prev);            \t\t// 2. track features\t\tcv::calcOpticalFlowPyrLK(gray_prev, gray, // 2 consecutive images\t\t\tpoints[0], // input point position in first image            points[1], // output point postion in the second image                status,    // tracking success                err);      // tracking error            // 3. loop over the tracked points to reject the undesirables            int k=0;            for( int i= 0; i &lt; points[1].size(); i++ ) &#123;                // do we keep this point?                if (acceptTrackedPoint(i)) &#123;                    // keep this point in vector                    initial[k]= initial[i];                    points[1][k++] = points[1][i];                &#125;            &#125;\t\t// eliminate unsuccesful points        points[1].resize(k);\t\tinitial.resize(k);        // 4. handle the accepted tracked points\t\thandleTrackedPoints(frame, output);        // 5. current points and image become previous ones\t\tstd::swap(points[1], points[0]);        cv::swap(gray_prev, gray);\t&#125;\t// feature point detection\tvoid detectFeaturePoints() &#123;\t\t\t\t\t// detect the features\t\tcv::goodFeaturesToTrack(gray, // the image \t\t\tfeatures,   // the output detected features\t\t\tmax_count,  // the maximum number of features \t\t\tqlevel,     // quality level\t\t\tminDist);   // min distance between two features\t&#125;\t// determine if new points should be added\tbool addNewPoints() &#123;\t\t// if too few points\t\treturn points[0].size()&lt;=10;\t&#125;\t// determine which tracked point should be accepted\t// here we keep only moving points\tbool acceptTrackedPoint(int i) &#123;\t\treturn status[i] &amp;&amp; // status is false if unable to track point i\t\t\t// if point has moved\t\t\t(abs(points[0][i].x-points[1][i].x)+\t\t\t(abs(points[0][i].y-points[1][i].y))&gt;2);\t&#125;\t// handle the currently tracked points\tvoid handleTrackedPoints(cv:: Mat &amp;frame, cv:: Mat &amp;output) &#123;\t\t// for all tracked points\t\tfor(int i= 0; i &lt; points[1].size(); i++ ) &#123;\t\t\t// draw line and circle\t\t    cv::line(output, initial[i], points[1][i], cv::Scalar(255,255,255));\t\t\tcv::circle(output, points[1][i], 3, cv::Scalar(255,255,255),-1);\t\t&#125;\t&#125;&#125;;#endif\n开始跟踪！！\nint main()&#123;\t// Create video procesor instance\tVideoProcessor processor;\t// Create feature tracker instance\tFeatureTracker tracker;\t// Open video file\tprocessor.setInput(&quot;bike.avi&quot;);\t// set frame processor\tprocessor.setFrameProcessor(&amp;tracker);\t// Declare a window to display the video\tprocessor.displayOutput(&quot;Tracked Features&quot;);\t// Play the video at the original frame rate\tprocessor.setDelay(1000./processor.getFrameRate());\tprocessor.stopAtFrameNo(90);\t// Start the process\tprocessor.run();\tcv::waitKey();&#125;\n\n估算光流// Drawing optical flow vectors on an imagevoid drawOpticalFlow(const cv::Mat&amp; oflow,    // the optical flow \t                 cv::Mat&amp; flowImage,      // the produced image\t                 int stride,  // the stride for displaying the vectors\t                 float scale, // multiplying factor for the vectors\t                 const cv::Scalar&amp; color) // the color of the vectors&#123;\t// create the image if required\tif (flowImage.size() != oflow.size()) &#123;\t\tflowImage.create(oflow.size(), CV_8UC3);\t\tflowImage = cv::Vec3i(255,255,255);\t&#125;\t// for all vectors using stride as a step\tfor (int y = 0; y &lt; oflow.rows; y += stride)\t\tfor (int x = 0; x &lt; oflow.cols; x += stride) &#123;\t\t\t// gets the vector\t\t\t\tcv::Point2f vector = oflow.at&lt; cv::Point2f&gt;(y, x);\t\t\t// draw the line\t\t\t\tcv::line(flowImage, cv::Point(x, y), \t\t\t\t     cv::Point(static_cast&lt;int&gt;(x + scale*vector.x + 0.5), \t\t\t\t\t\t       static_cast&lt;int&gt;(y + scale*vector.y + 0.5)), color);\t\t\t// draw the arrow tip\t\t\t\tcv::circle(flowImage, cv::Point(static_cast&lt;int&gt;(x + scale*vector.x + 0.5),\t\t\t\t                            static_cast&lt;int&gt;(y + scale*vector.y + 0.5)), 1, color, -1);\t\t&#125;&#125;int main()&#123;\t// pick 2 frames of the sequence\tcv::Mat frame1= cv::imread(&quot;goose/goose230.bmp&quot;, 0);\tcv::Mat frame2= cv::imread(&quot;goose/goose237.bmp&quot;, 0);\t// Combined display\tcv::Mat combined(frame1.rows, frame1.cols + frame2.cols, CV_8U);\tframe1.copyTo(combined.colRange(0, frame1.cols));\tframe2.copyTo(combined.colRange(frame1.cols, frame1.cols+frame2.cols));\tcv::imshow(&quot;Frames&quot;, combined);\t// Create the optical flow algorithm\tcv::Ptr&lt;cv::DualTVL1OpticalFlow&gt; tvl1 = cv::createOptFlow_DualTVL1();\tstd::cout &lt;&lt; &quot;regularization coeeficient: &quot; &lt;&lt; tvl1-&gt;getLambda() &lt;&lt; std::endl; // the smaller the soomther\tstd::cout &lt;&lt; &quot;Number of scales: &quot; &lt;&lt; tvl1-&gt;getScalesNumber() &lt;&lt; std::endl; // number of scales\tstd::cout &lt;&lt; &quot;Scale step: &quot; &lt;&lt; tvl1-&gt;getScaleStep() &lt;&lt; std::endl; // size between scales\tstd::cout &lt;&lt; &quot;Number of warpings: &quot; &lt;&lt; tvl1-&gt;getWarpingsNumber() &lt;&lt; std::endl; // size between scales\tstd::cout &lt;&lt; &quot;Stopping criteria: &quot; &lt;&lt; tvl1-&gt;getEpsilon() &lt;&lt; &quot; and &quot; &lt;&lt; tvl1-&gt;getOuterIterations() &lt;&lt; std::endl; // size between scales\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// compute the optical flow between 2 frames\tcv::Mat oflow; // image of 2D flow vectors\t// compute optical flow between frame1 and frame2\ttvl1-&gt;calc(frame1, frame2, oflow);\t// Draw the optical flow image\tcv::Mat flowImage;\tdrawOpticalFlow(oflow,     // input flow vectors \t\tflowImage, // image to be generated\t\t8,         // display vectors every 8 pixels\t\t2,         // multiply size of vectors by 2\t\tcv::Scalar(0, 0, 0)); // vector color\tcv::imshow(&quot;Optical Flow&quot;, flowImage);\t// compute a smoother optical flow between 2 frames\ttvl1-&gt;setLambda(0.075);\ttvl1-&gt;calc(frame1, frame2, oflow);\t// Draw the optical flow image\tcv::Mat flowImage2;\tdrawOpticalFlow(oflow,     // input flow vectors \t\tflowImage2, // image to be generated\t\t8,         // display vectors every 8 pixels\t\t2,         // multiply size of vectors by 2\t\tcv::Scalar(0, 0, 0)); // vector color\tcv::imshow(&quot;Smoother Optical Flow&quot;, flowImage2);\tcv::waitKey();&#125;\n\n\n跟踪视频中的物体visualTracker.h\n#if !defined FTRACKER#define FTRACKER#include &lt;string&gt;#include &lt;vector&gt;#include &lt;opencv2/core.hpp&gt;#include &lt;opencv2/highgui.hpp&gt;#include &lt;opencv2/imgproc.hpp&gt;#include &lt;opencv2/features2d.hpp&gt;#include &lt;opencv2/tracking/tracker.hpp&gt;#include &quot;videoprocessor.h&quot;class VisualTracker : public FrameProcessor &#123;\t\tcv::Ptr&lt;cv::Tracker&gt; tracker;\tcv::Rect2d box;\tbool reset;  public:\t// constructor specifying the tracker to be used\tVisualTracker(cv::Ptr&lt;cv::Tracker&gt; tracker) : \t\t             reset(true), tracker(tracker) &#123;&#125;\t// set the bounding box to initiate tracking\tvoid setBoundingBox(const cv::Rect2d&amp; bb) &#123;\t\tbox = bb;\t\treset = true;\t&#125;\t\t// callback processing method\tvoid process(cv:: Mat &amp;frame, cv:: Mat &amp;output) &#123;\t\tif (reset) &#123; // new tracking session\t\t\treset = false;\t\t\ttracker-&gt;init(frame, box);\t\t&#125; else &#123; // update the target&#x27;s position\t\t\t\t\ttracker-&gt;update(frame, box);\t\t&#125;\t\t// draw bounding box on current frame\t\tframe.copyTo(output);\t\tcv::rectangle(output, box, cv::Scalar(255, 255, 255), 2);\t&#125;&#125;;#endif\nint main()&#123;\t// Create video procesor instance\tVideoProcessor processor;\t\t// generate the filename\tstd::vector&lt;std::string&gt; imgs;\tstd::string prefix = &quot;goose/goose&quot;;\tstd::string ext = &quot;.bmp&quot;;\t// Add the image names to be used for tracking\tfor (long i = 130; i &lt; 317; i++) &#123;\t\tstd::string name(prefix);\t\tstd::ostringstream ss; ss &lt;&lt; std::setfill(&#x27;0&#x27;) &lt;&lt; std::setw(3) &lt;&lt; i; name += ss.str();\t\tname += ext;\t\tstd::cout &lt;&lt; name &lt;&lt; std::endl;\t\timgs.push_back(name);\t&#125;\t// Create feature tracker instance\tcv::Ptr&lt;cv::TrackerMedianFlow&gt; ptr= cv::TrackerMedianFlow::createTracker();\tVisualTracker tracker(ptr);\t// VisualTracker tracker(cv::TrackerKCF::createTracker());\t// Open video file\tprocessor.setInput(imgs);\t// set frame processor\tprocessor.setFrameProcessor(&amp;tracker);\t// Declare a window to display the video\tprocessor.displayOutput(&quot;Tracked object&quot;);\t// Define the frame rate for display\tprocessor.setDelay(50);\t// Specify the original target position\tcv::Rect bb(290, 100, 65, 40);\ttracker.setBoundingBox(bb);\t// Start the tracking\tprocessor.run();\tcv::waitKey();\t// Illustration of the Median Tracker principle\tcv::Mat image1 = cv::imread(&quot;goose/goose130.bmp&quot;, cv::ImreadModes::IMREAD_GRAYSCALE);\t// define a regular grid of points\tstd::vector&lt;cv::Point2f&gt; grid;\tfor (int i = 0; i &lt; 10; i++) &#123;\t\tfor (int j = 0; j &lt; 10; j++) &#123;\t\t\tcv::Point2f p(bb.x+i*bb.width/10.,bb.y+j*bb.height/10);\t\t\tgrid.push_back(p);\t\t&#125;\t&#125;\t// track in next image\tcv::Mat image2 = cv::imread(&quot;goose/goose131.bmp&quot;, cv::ImreadModes::IMREAD_GRAYSCALE);\tstd::vector&lt;cv::Point2f&gt; newPoints;\tstd::vector&lt;uchar&gt; status; // status of tracked features\tstd::vector&lt;float&gt; err;    // error in tracking\t// track the points\tcv::calcOpticalFlowPyrLK(image1, image2, // 2 consecutive images\t\tgrid,      // input point position in first image\t\tnewPoints, // output point postion in the second image\t\tstatus,    // tracking success\t\terr);      // tracking error\t// Draw the points\tfor (cv::Point2f p : grid) &#123;\t\tcv::circle(image1, p, 1, cv::Scalar(255, 255, 255), -1);\t&#125;\tcv::imshow(&quot;Initial points&quot;, image1);\tfor (cv::Point2f p : newPoints) &#123;\t\tcv::circle(image2, p, 1, cv::Scalar(255, 255, 255), -1);\t&#125;\tcv::imshow(&quot;Tracked points&quot;, image2);\tcv::waitKey();&#125;\n\n\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"OpenCV (学长走路背景)","url":"/2021/08/06/OpenCVProject2/","content":"小作业——继续跟踪\n\n这次的背景有一只学长路过\n\n所以不能用动态检测，只能用颜色检测啦！！！\n（ps. 动态检测是啥，其实我也没了解很多，我以为大家都在用颜色检测，后来才知道学长学姐上一个小作业是用动态检测的，所以问学长要了动态检测的代码\n#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui.hpp&gt;#include &lt;opencv2/video/background_segm.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;const int Train = 50;int main()&#123;\tPtr&lt;BackgroundSubtractorMOG2&gt; mog = createBackgroundSubtractorMOG2(100, 25, false); // 背景消去 \t//bgsubtractor-&gt;setVarThreshold(20);\tMat foreGround;\tMat backGround;\tint trainCounter = 0;\tbool dynamicDetect = true;\tnamedWindow(&quot;src&quot;, WINDOW_FREERATIO);\tnamedWindow(&quot;foreground&quot;, WINDOW_FREERATIO);\tVideoCapture cap(&quot;Resources/task3.mp4&quot;); \tif (!cap.isOpened())\t&#123;\t\tcout &lt;&lt; &quot;Fail to open!&quot;&lt;&lt; endl;\t\treturn -1;\t&#125;\tMat src;\tMat kernel = getStructuringElement(MORPH_RECT, Size(3, 3)); // 开操作去噪声 \tbool stop = false;\twhile (!stop)\t&#123;\t\tcap &gt;&gt; src;\t\tif (src.empty())\t\t\tbreak;\t\tif (dynamicDetect)\t\t&#123;\t\t\tmog-&gt;apply(src, foreGround, 0.005);\t\t\tmorphologyEx(foreGround, foreGround, MORPH_OPEN, kernel);\t\t\t//图像处理过程\t\t\tmedianBlur(foreGround, foreGround, 3);\t\t\tdilate(foreGround, foreGround, Mat(), Point(-1, -1), 3);\t\t\terode(foreGround, foreGround, Mat(), Point(-1, -1), 6);\t\t\tdilate(foreGround, foreGround, Mat(), Point(-1, -1), 3);\t\t\timshow(&quot;foreground&quot;, foreGround);\t\t\tif (trainCounter &lt; Train)//训练期间所得结果为不准确结果，不应作为后续\t\t\t&#123;\t\t\t\tMat findc;\t\t\t\tforeGround.copyTo(findc);\t\t\t\tvector&lt;vector&lt;Point&gt;&gt; contours;\t\t\t\tfindContours(findc, contours, RETR_EXTERNAL, CHAIN_APPROX_NONE);        //寻找轮廓\t\t\t\t//targets.clear();\t\t\t\tconst int maxArea = 200;\t\t\t\tsize_t s = contours.size();\t\t\t\tfor (size_t i = 0; i &lt; s; i++)\t\t\t\t&#123;\t\t\t\t\tdouble area = abs(contourArea(contours[i]));\t\t\t\t\tif (area &gt; maxArea)\t\t\t\t\t&#123;\t\t\t\t\t\tRect mr = boundingRect(Mat(contours[i]));\t\t\t\t\t\trectangle(src, mr, Scalar(0, 0, 255), 2, 8, 0);\t\t\t\t\t\t//targets.push_back(mr);\t\t\t\t\t&#125;\t\t\t\t&#125;\t\t\t\t//string text;\t\t\t\t\t\t\t\t\tchar text[50];\t\t\t\tsprintf_s(text, &quot;background training -%d- ...&quot;, trainCounter);\t\t\t\tputText(src, text, Point(50, 50), 3, 1, Scalar(0, 255, 255), 2, 8, false);\t\t\t\t//delete[] text;\t\t\t&#125;\t\t\telse\t\t\t&#123;\t\t\t\t//detects.clear();\t\t\t\tMat findc;\t\t\t\tforeGround.copyTo(findc);\t\t\t\tvector&lt;vector&lt;Point&gt;&gt; contours;\t\t\t\tcv::findContours(findc, contours, RETR_EXTERNAL, CHAIN_APPROX_NONE);\t\t\t\tconst int maxArea = 200;\t\t\t\tsize_t s = contours.size();\t\t\t\tRNG rng;\t\t\t\t\tfor (size_t i = 0; i &lt; s; i++)\t\t\t\t\t&#123;\t\t\t\t\t\tdouble area = abs(contourArea(contours[i]));\t\t\t\t\t\tif (area &gt; maxArea)\t\t\t\t\t\t&#123;\t\t\t\t\t\t\tScalar sca_color = Scalar(rng.uniform(0, 256), rng.uniform(0, 256), rng.uniform(0, 256));\t\t\t\t\t\t\tRect mr = boundingRect(Mat(contours[i]));\t\t\t\t\t\t\trectangle(src, mr, sca_color, 2, 8, 0);\t\t\t\t\t\t\t//可以对动态目标进行相应操作\t\t\t\t\t\t&#125;\t\t\t\t\t&#125;\t\t\t&#125;\t\t\ttrainCounter++;\t\t&#125;\t\timshow(&quot;src&quot;, src);\t\tif (waitKey(30) == 27) //Esc键退出    \t\t&#123;\t\t\tstop = true;\t\t&#125;\t&#125;\treturn 0;&#125;\n另附相关资料\nhttps://blog.csdn.net/m0_37901643/article/details/72841289\n%完了动态检测，还是学习一波颜色检测吧 ~ ~\n冲冲冲！！！\nCode（根据之前颜色检测的代码来魔改\n首先 需要用 colorPicker 检测出需要检测的颜色的 HSV 三个分量的最大最小值。\n然后就可以改代码了\n#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;#include&lt;opencv2/core.hpp&gt;\tusing namespace std;using namespace cv;Mat img;vector&lt;vector&lt;int&gt;&gt; newPoints;vector&lt;vector&lt;int&gt;&gt; myColors&#123; &#123;0,142,120,179,255,255&#125;, // 橙色（hmin smin vmin hmax smax vmax）\t\t\t\t\t\t\t\t&#123;0,0,0,0,0,0&#125; &#125;; // 绿色（hmin smin vmin hmax smax vmax）vector&lt;Scalar&gt; myColorValues&#123; &#123;255,0,255&#125;, // 蓝色\t\t\t\t\t\t\t\t&#123;0,255,0&#125; &#125;; // 绿色//! 获取轮廓Point getContours(Mat imgDil) &#123; // imgDil是传入的扩张边缘的图像用来查找轮廓，img是要在其上绘制轮廓的图像\tvector&lt;vector&lt;Point&gt;&gt; contours; // 轮廓检测到的轮廓。每个轮廓线存储为一个点的向量\tvector&lt;Vec4i&gt; hierarchy; // 包含关于映像拓扑的信息  typedef Vec&lt;int, 4&gt; Vec4i;具有4个整数值\t//在二值图像中查找轮廓。该函数利用该算法从二值图像中提取轮廓\tfindContours(imgDil, contours, hierarchy, RETR_LIST, CHAIN_APPROX_SIMPLE);\t//drawContours(img, contours, -1, Scalar(255, 0, 255), 2); // img：要绘制轮廓在什么图片上，contours：要绘制的轮廓，-1定义要绘制的轮廓号（-1表示所有轮廓），Saclar表示轮廓颜色，2表示厚度\tvector&lt;vector&lt;Point&gt;&gt; conPoly(contours.size()); // conploy的数量应小于contours\tvector&lt;Rect&gt; boundRect(contours.size());\tPoint myPoint(0, 0);\t//过滤器：通过轮廓面积来过滤噪声\tfor (int i = 0; i &lt; contours.size(); i++) &#123; // 遍历检测到的轮廓\t\tdouble area = contourArea(contours[i]);\t\t// cout &lt;&lt; area &lt;&lt; endl;\t\tstring objectType;\t\tif (area &gt; 10) &#123; // 轮廓面积＞10才绘制\t\t\t// 计算轮廓周长或凹坑长度。该函数计算了曲线长度和封闭的周长。\t\t\tdouble peri = arcLength(contours[i], true); // 计算封闭轮廓周长\t\t\tapproxPolyDP(contours[i], conPoly[i], 0.02 * peri, true); // 以指定的精度近似多边形曲线。第二个参数conPloy[i]存储近似的结果，是输出。\t\t\tboundRect[i] = boundingRect(conPoly[i]); // 计算边界矩形\t\t\tmyPoint.x = boundRect[i].x;\t\t\tmyPoint.y = boundRect[i].y + boundRect[i].height / 2;\t\t\tif (conPoly[i].size() &gt; 4)\t\t\t\tputText(img, &quot;Small Ball&quot;, Point(boundRect[i].x, boundRect[i].y - 10), FONT_HERSHEY_PLAIN, 3, Scalar(0, 255, 0), 6);\t\t\t/*绘制边界矩形*/\t\t\trectangle(img, boundRect[i].tl(), boundRect[i].br(), Scalar(0, 255, 0), 5); // tl()：topleft矩形左上角坐标 br()：bottom right矩形右下角坐标\t\t\tdrawContours(img, conPoly, i, Scalar(255, 0, 0), 2);\t\t&#125;\t&#125;\treturn myPoint;&#125;vector&lt;vector&lt;int&gt;&gt; findColor(Mat img) &#123;\tMat imgHSV;\tcvtColor(img, imgHSV, COLOR_BGR2HSV); // 转换图像到HSV空间，在其中查找颜色更加容易\tfor (int i = 0; i &lt; myColors.size(); i++)\t&#123;\t\tScalar lower(myColors[i][0], myColors[i][1], myColors[i][2]);\t\tScalar upper(myColors[i][3], myColors[i][4], myColors[i][5]);\t\tMat mask;\t\tinRange(imgHSV, lower, upper, mask); // 定义颜色下限和上限，因为由于照明和不同的阴影，颜色的值将不完全相同，会是一个值的范围\t\t// imshow(to_string(i), mask);\t\tPoint myPoint = getContours(mask);\t\tif (myPoint.x != 0 &amp;&amp; myPoint.y != 0) &#123; // 没检测到东西的时候就不加入新点\t\t\tnewPoints.push_back(&#123; myPoint.x,myPoint.y,i &#125;); // i为颜色索引\t\t&#125;\t&#125;\treturn newPoints;&#125;void drawOnCanvas(vector&lt;vector&lt;int&gt;&gt; newPoints, vector&lt;Scalar&gt; myColorValues) &#123;\tfor (int i = 1; i &lt; newPoints.size(); i++) &#123;\t\tline(img, Point(newPoints[i - 1][0], newPoints[i - 1][1]), Point(newPoints[i][0], newPoints[i][1]), Scalar(0, 255, 255), 10);\t&#125;&#125;int main() &#123;\tVideoCapture cap(&quot;D:/Summer Holiday Practice/opencv learning/Resources/TrackBall2.mp4&quot;);\tif (!cap.isOpened()) &#123;\t\tcout &lt;&lt; &quot;fail to open!&quot; &lt;&lt; endl;\t\treturn -1;\t&#125;\tchar c;\tbool stop = false;\tnamedWindow(&quot;Image&quot;, WINDOW_FREERATIO);\twhile (c = waitKey(1))\t&#123;\t\tif (c == 27)\t\t\tbreak;\t\tif (c == &#x27;p&#x27;)\t\t&#123;\t\t\tstop = !stop;\t\t&#125;\t\tif (!stop)\t\t&#123;\t\t\tcap &gt;&gt; img;\t\t\tnewPoints = findColor(img);\t\t\tdrawOnCanvas(newPoints, myColorValues);\t\t\timshow(&quot;Image&quot;, img);\t\t&#125;\t&#125;\treturn 0;&#125;\n追踪效果如图：\n\n学了点儿啥呢主要还是在 colorPicker 的使用上\nMat imgHSV, mask;int hmin = 0, smin = 0, vmin = 0;int hmax = 179, smax = 255, vmax = 255;void main() &#123;\tnamedWindow(&quot;Trackbars&quot;, (640, 200));\tcreateTrackbar(&quot;Hue Min&quot;, &quot;Trackbars&quot;, &amp;hmin, 179);\tcreateTrackbar(&quot;Hue Max&quot;, &quot;Trackbars&quot;, &amp;hmax, 179);\tcreateTrackbar(&quot;Sat Min&quot;, &quot;Trackbars&quot;, &amp;smin, 255);\tcreateTrackbar(&quot;Sat Max&quot;, &quot;Trackbars&quot;, &amp;smax, 255);\tcreateTrackbar(&quot;Val Min&quot;, &quot;Trackbars&quot;, &amp;vmin, 255);\tcreateTrackbar(&quot;Val Max&quot;, &quot;Trackbars&quot;, &amp;vmax, 255);\twhile (true) &#123;\t\tstring path = &quot;D:/Summer Holiday Practice/opencv learning/Resources/ball2.png&quot;;\t\tMat img = imread(path);\t\t/*//  转换单通道\t\tif (img.channels() == 4) &#123;\t\t\tcv::cvtColor(img, imgHSV, COLOR_BGRA2GRAY);\t\t&#125;\t\telse if (img.channels() == 3) &#123;\t\t\tcv::cvtColor(img, imgHSV, COLOR_BGR2GRAY);\t\t&#125;\t\telse if (img.channels() == 2) &#123;\t\t\tcv::cvtColor(img, imgHSV, COLOR_BGR5652GRAY);\t\t&#125;\t\telse if (img.channels() == 1) &#123;// 单通道的图片直接就不需要处理\t\t\timgHSV = img;\t\t&#125;\t\telse &#123; // 负数,说明图有问题 直接返回\t\t\treturn;\t\t&#125;*/\t\tcvtColor(img, imgHSV, COLOR_BGR2HSV);\t\tScalar lower(hmin, smin, vmin);\t\tScalar upper(hmax, smax, vmax);\t\tinRange(imgHSV, lower, upper, mask);\t\tcout &lt;&lt; hmin &lt;&lt; &#x27;,&#x27; &lt;&lt; smin &lt;&lt; &#x27;,&#x27; &lt;&lt; vmin &lt;&lt; &#x27;,&#x27; &lt;&lt; hmax &lt;&lt; &#x27;,&#x27; &lt;&lt; smax &lt;&lt; &#x27;,&#x27; &lt;&lt; vmax &lt;&lt; endl;\t\tnamedWindow(&quot;Image&quot;, WINDOW_NORMAL);\t\tnamedWindow(&quot;Image mask&quot;, WINDOW_NORMAL);\t\timshow(&quot;Image&quot;, img);\t\timshow(&quot;Image mask&quot;, mask);\t\twaitKey(1);\t&#125;&#125;\nPath一是路径书写上，我总是会遇到无法正常打开图片或者视频的情况，且大部分情况下，都与路径书写有关，也试过把相对路径改成绝对路径。\n经过实践后，只能说，答案如下， D:/Summer Holiday Practice/opencv learning/Resources/ball2.png \ncvtColor之前 cvtColor 一直报错，后来去找资料，资料上说 cvtColor 不能处理灰度图，上面代码中注释掉的那一段，是在判断图片的通道数，并进行转换。\n虽然我并不是因为这个原因，但这也是值得注意的一个地方吧。\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"Speech_Re-gifted","url":"/2021/08/08/Speech-Re-gifted/","content":"Practice makes perfect ~ ~\n\nToday, I’d like to begin with two pictures. One is a delicate purple gift  in the shape of an egg standing on a shelf, with the foreground saying “re-gifted”. The other is the very gift lying on the ground in pieces who shows her inner world, a carousel singing bitterly. Yes, it’s two pictures in this movie. To me, the movie not only presents how misery the gift’s fate is, but also acts as a timeless reminder that we should not ignore the need of others and be indifferent to the beauty inside others.\nIn 2017, I went to a orphanage and spent a whole week with children there. In seven weeks I built a strong band between a girl and me. She was abandoned when she was born due to the illness. At the first day, she seldom spoke even a word, and refused any of our help but always smiled. She was just like that gift to me, beautiful — her loveliness but fragile — her silentness. On of the adult said she was once adopted but was abandoned again due to her rejection to everything. I don’t know how to help her. I chose to be her side silently. Just let her know I was there. And whenever I had chances to speak with her, I would share with her many things in my life and asked her something similar and her emotions in those cases. Day by day, she was not that kind of autistic. I’m very happy to know that she liked playing flute and encouraged her to perform in front of many people.\n(Many years have passed, she still isn’t adopted just like what she once told me that she doesn’t want to be adopted because she will lose friends here.)\nEvery human is a gift from god. Every human living in the world has the need to be listened and cared, no matter how ugly or beautiful he is, no matter whether he is healthy or not. Inside a tough egg shell, it’s a music box voting for himself. To avoid more adorable souls break into pieces, we should find out their inner beauty and let it shine in front of you and in front of me. Thank you.\n","categories":["HDU's Learning"],"tags":["English speech"]},{"title":"OpenCV (背景单一小球视频跟踪)","url":"/2021/07/29/OpenCVProject1/","content":"小作业—视频跟踪\n\n  我们录制了一段视频，视频内容如下：\n一个红色（可能是橙色……）的小球，从地上滚过去\n视频特征：1. 背景单一 2. 要捕捉的对象与背景具有明显的颜色差别\n学长的初步代码终于能跑起来了！！！\n#include &quot;opencv2/opencv.hpp&quot;#include &quot;opencv2/video/background_segm.hpp&quot;#include&lt;iostream&gt;using namespace std;using namespace cv;void main() &#123;\t\tVideoCapture video(&quot;D:/Summer Holiday Practice/opencv learning/Resources/tast.mp4&quot;);\t\tint frameNum = 1;\t\tMat frame, mask, thresholdImage, output;\t\tif (!video.isOpened())\t\t\tcout &lt;&lt; &quot;fail to open!&quot; &lt;&lt; endl;\t\t//cout&lt;&lt;video.isOpened();\t\tdouble totalFrameNumber = video.get(CAP_PROP_FRAME_COUNT);\t\tvideo &gt;&gt; frame;\t\tPtr&lt;BackgroundSubtractorMOG2&gt; bgsubtractor = createBackgroundSubtractorMOG2();\t\tbgsubtractor-&gt;setVarThreshold(20);\t\twhile (true) &#123;\t\t\tif (totalFrameNumber == frameNum)\t\t\t\tbreak;\t\t\tvideo &gt;&gt; frame;\t\t\t++frameNum;\t\t\t//bgSubtractor(frame, mask, 0.001);\t\t\tbgsubtractor-&gt;apply(frame, mask, 0.01);\t\t\timshow(&quot;mask&quot;, mask);\t\t\twaitKey(10);\t\t&#125;&#125;\n学长的最终代码\n学长不愧是学长，太巨了！！！！\n#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;#include&lt;opencv2/core.hpp&gt;\tusing namespace std;using namespace cv;/// &lt;summary&gt;/// Project 1/// 使用HSV空间检测颜色、找到轮廓所在位置、取轮廓的位置然后创建一个圆/// &lt;/summary&gt;Mat img;vector&lt;vector&lt;int&gt;&gt; newPoints;vector&lt;vector&lt;int&gt;&gt; myColors&#123; &#123;0,87,54,15,205,245&#125;,//橙色（hmin smin vmin hmax smax vmax）\t\t\t\t\t\t\t\t&#123;0,0,0,0,0,0&#125; &#125;;//绿色（hmin smin vmin hmax smax vmax）vector&lt;Scalar&gt; myColorValues&#123; &#123;255,0,255&#125;,//蓝色\t\t\t\t\t\t\t\t&#123;0,255,0&#125; &#125;;//绿色//! 获取轮廓Point getContours(Mat imgDil) &#123;//imgDil是传入的扩张边缘的图像用来查找轮廓，img是要在其上绘制轮廓的图像\tvector&lt;vector&lt;Point&gt;&gt; contours;//轮廓检测到的轮廓。每个轮廓线存储为一个点的向量\tvector&lt;Vec4i&gt; hierarchy;//包含关于映像拓扑的信息  typedef Vec&lt;int, 4&gt; Vec4i;具有4个整数值\t//在二值图像中查找轮廓。该函数利用该算法从二值图像中提取轮廓\tfindContours(imgDil, contours, hierarchy, RETR_LIST, CHAIN_APPROX_SIMPLE);\t//drawContours(img, contours, -1, Scalar(255, 0, 255), 2);//img：要绘制轮廓在什么图片上，contours：要绘制的轮廓，-1定义要绘制的轮廓号（-1表示所有轮廓），Saclar表示轮廓颜色，2表示厚度\t\tvector&lt;vector&lt;Point&gt;&gt; conPoly(contours.size());//conploy的数量应小于contours\tvector&lt;Rect&gt; boundRect(contours.size());\tPoint myPoint(0, 0);\t//过滤器：通过轮廓面积来过滤噪声\tfor (int i = 0; i &lt; contours.size(); i++) &#123;//遍历检测到的轮廓\t\tint area = contourArea(contours[i]);\t\t\t\t//cout &lt;&lt; area &lt;&lt; endl;\t\tstring objectType;\t\tif (area &gt; 1000) &#123;//轮廓面积＞1000才绘制\t\t\t//计算轮廓周长或凹坑长度。该函数计算了曲线长度和封闭的周长。\t\t\tfloat peri = arcLength(contours[i], true);//计算封闭轮廓周长\t\t\tapproxPolyDP(contours[i], conPoly[i], 0.02 * peri, true);//以指定的精度近似多边形曲线。第二个参数conPloy[i]存储近似的结果，是输出。\t\t\tboundRect[i] = boundingRect(conPoly[i]);//计算边界矩形\t\t\tmyPoint.x = boundRect[i].x;\t\t\tmyPoint.y = boundRect[i].y + boundRect[i].height / 2;\t\t\tif (conPoly[i].size() &gt; 4)\t\t\t\tputText(img, &quot;Small Ball&quot;, Point(boundRect[i].x, boundRect[i].y - 10), FONT_HERSHEY_PLAIN ,3 , Scalar(0, 255, 0), 6);\t\t\t/*绘制边界矩形*/\t\t\trectangle(img, boundRect[i].tl(), boundRect[i].br(), Scalar(0, 255, 0), 5);//tl()：topleft矩形左上角坐标 br()：bottom right矩形右下角坐标\t\t\tdrawContours(img, conPoly, i, Scalar(255, 0, 0), 2);\t\t&#125;\t&#125;\treturn myPoint;&#125;vector&lt;vector&lt;int&gt;&gt; findColor(Mat img) &#123;\tMat imgHSV;\tcvtColor(img, imgHSV, COLOR_BGR2HSV);//转换图像到HSV空间，在其中查找颜色更加容易\tfor (int i = 0; i &lt; myColors.size(); i++)\t&#123;\t\tScalar lower(myColors[i][0], myColors[i][1], myColors[i][2]);\t\tScalar upper(myColors[i][3], myColors[i][4], myColors[i][5]);\t\tMat mask;\t\tinRange(imgHSV, lower, upper, mask);//定义颜色下限和上限，因为由于照明和不同的阴影，颜色的值将不完全相同，会是一个值的范围\t\t//imshow(to_string(i), mask);\t\tPoint myPoint=getContours(mask);\t\tif (myPoint.x != 0 &amp;&amp; myPoint.y != 0) &#123;//没检测到东西的时候就不加入新点\t\t\tnewPoints.push_back(&#123; myPoint.x,myPoint.y,i &#125;);//i为颜色索引\t\t&#125;\t&#125;\treturn newPoints;\t&#125;void drawOnCanvas(vector&lt;vector&lt;int&gt;&gt; newPoints, vector&lt;Scalar&gt; myColorValues) &#123;\tfor (int i = 1; i &lt; newPoints.size(); i++) &#123;\t\tline(img, Point(newPoints[i - 1][0], newPoints[i - 1][1]), Point(newPoints[i][0], newPoints[i][1]), Scalar(0, 255, 255), 10);\t&#125;&#125;void main() &#123;\tVideoCapture cap(&quot;Resources/task.mp4&quot;);//相机id=0\tchar c;\tbool stop = false;\tnamedWindow(&quot;Image&quot;, WINDOW_FREERATIO);\twhile (c = waitKey(1))\t&#123;\t\tif (c == 27)\t\t\tbreak;\t\tif (c == &#x27;p&#x27;)\t\t&#123;\t\t\tstop = !stop;\t\t&#125;\t\tif (!stop)\t\t&#123;\t\t\tcap &gt;&gt; img;\t\t\tnewPoints = findColor(img);\t\t\tdrawOnCanvas(newPoints, myColorValues);\t\t\timshow(&quot;Image&quot;, img);\t\t&#125;\t&#125;&#125;\n我首先更改了视频读取的路径\nD:/Summer Holiday Practice/opencv learning/Resources/task2.mp4\n编译运行后，视频闪退了！？？！\n随后，调整了调试属性。平台从 x64活动 调整为 x64\n在我的电脑上最终代码如下\n#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;#include&lt;opencv2/core.hpp&gt;\tusing namespace std;using namespace cv;/// &lt;summary&gt;/// Project 1/// 使用HSV空间检测颜色、找到轮廓所在位置、取轮廓的位置然后创建一个圆/// &lt;/summary&gt;Mat img;vector&lt;vector&lt;int&gt;&gt; newPoints;vector&lt;vector&lt;int&gt;&gt; myColors&#123; &#123;0,87,54,15,205,245&#125;,//橙色（hmin smin vmin hmax smax vmax）\t\t\t\t\t\t\t\t&#123;0,0,0,0,0,0&#125; &#125;;//绿色（hmin smin vmin hmax smax vmax）vector&lt;Scalar&gt; myColorValues&#123; &#123;255,0,255&#125;,//蓝色\t\t\t\t\t\t\t\t&#123;0,255,0&#125; &#125;;//绿色//! 获取轮廓Point getContours(Mat imgDil) &#123;//imgDil是传入的扩张边缘的图像用来查找轮廓，img是要在其上绘制轮廓的图像\tvector&lt;vector&lt;Point&gt;&gt; contours;//轮廓检测到的轮廓。每个轮廓线存储为一个点的向量\tvector&lt;Vec4i&gt; hierarchy;//包含关于映像拓扑的信息  typedef Vec&lt;int, 4&gt; Vec4i;具有4个整数值\t//在二值图像中查找轮廓。该函数利用该算法从二值图像中提取轮廓\tfindContours(imgDil, contours, hierarchy, RETR_LIST, CHAIN_APPROX_SIMPLE);\t//drawContours(img, contours, -1, Scalar(255, 0, 255), 2);//img：要绘制轮廓在什么图片上，contours：要绘制的轮廓，-1定义要绘制的轮廓号（-1表示所有轮廓），Saclar表示轮廓颜色，2表示厚度\tvector&lt;vector&lt;Point&gt;&gt; conPoly(contours.size());//conploy的数量应小于contours\tvector&lt;Rect&gt; boundRect(contours.size());\tPoint myPoint(0, 0);\t//过滤器：通过轮廓面积来过滤噪声\tfor (int i = 0; i &lt; contours.size(); i++) &#123;//遍历检测到的轮廓\t\tdouble area = contourArea(contours[i]);\t\t//cout &lt;&lt; area &lt;&lt; endl;\t\tstring objectType;\t\tif (area &gt; 1000) &#123;//轮廓面积＞1000才绘制\t\t\t//计算轮廓周长或凹坑长度。该函数计算了曲线长度和封闭的周长。\t\t\tdouble peri = arcLength(contours[i], true);//计算封闭轮廓周长\t\t\tapproxPolyDP(contours[i], conPoly[i], 0.02 * peri, true);//以指定的精度近似多边形曲线。第二个参数conPloy[i]存储近似的结果，是输出。\t\t\tboundRect[i] = boundingRect(conPoly[i]);//计算边界矩形\t\t\tmyPoint.x = boundRect[i].x;\t\t\tmyPoint.y = boundRect[i].y + boundRect[i].height / 2;\t\t\tif (conPoly[i].size() &gt; 4)\t\t\t\tputText(img, &quot;Small Ball&quot;, Point(boundRect[i].x, boundRect[i].y - 10), FONT_HERSHEY_PLAIN, 3, Scalar(0, 255, 0), 6);\t\t\t/*绘制边界矩形*/\t\t\trectangle(img, boundRect[i].tl(), boundRect[i].br(), Scalar(0, 255, 0), 5);//tl()：topleft矩形左上角坐标 br()：bottom right矩形右下角坐标\t\t\tdrawContours(img, conPoly, i, Scalar(255, 0, 0), 2);\t\t&#125;\t&#125;\treturn myPoint;&#125;vector&lt;vector&lt;int&gt;&gt; findColor(Mat img) &#123;\tMat imgHSV;\tcvtColor(img, imgHSV, COLOR_BGR2HSV);//转换图像到HSV空间，在其中查找颜色更加容易\tfor (int i = 0; i &lt; myColors.size(); i++)\t&#123;\t\tScalar lower(myColors[i][0], myColors[i][1], myColors[i][2]);\t\tScalar upper(myColors[i][3], myColors[i][4], myColors[i][5]);\t\tMat mask;\t\tinRange(imgHSV, lower, upper, mask);//定义颜色下限和上限，因为由于照明和不同的阴影，颜色的值将不完全相同，会是一个值的范围\t\t//imshow(to_string(i), mask);\t\tPoint myPoint = getContours(mask);\t\tif (myPoint.x != 0 &amp;&amp; myPoint.y != 0) &#123;//没检测到东西的时候就不加入新点\t\t\tnewPoints.push_back(&#123; myPoint.x,myPoint.y,i &#125;);//i为颜色索引\t\t&#125;\t&#125;\treturn newPoints;&#125;void drawOnCanvas(vector&lt;vector&lt;int&gt;&gt; newPoints, vector&lt;Scalar&gt; myColorValues) &#123;\tfor (int i = 1; i &lt; newPoints.size(); i++) &#123;\t\tline(img, Point(newPoints[i - 1][0], newPoints[i - 1][1]), Point(newPoints[i][0], newPoints[i][1]), Scalar(0, 255, 255), 10);\t&#125;&#125;int main() &#123;\tVideoCapture cap(&quot;D:/Summer Holiday Practice/opencv learning/Resources/tast.mp4&quot;);//相机id=0\tif (!cap.isOpened()) &#123;\t\tcout &lt;&lt; &quot;fail to open!&quot; &lt;&lt; endl;\t\treturn -1;\t&#125;\tchar c;\tbool stop = false;\tnamedWindow(&quot;Image&quot;, WINDOW_FREERATIO);\twhile (c = waitKey(1))\t&#123;\t\tif (c == 27)\t\t\tbreak;\t\tif (c == &#x27;p&#x27;)\t\t&#123;\t\t\tstop = !stop;\t\t&#125;\t\tif (!stop)\t\t&#123;\t\t\tcap &gt;&gt; img;\t\t\tnewPoints = findColor(img);\t\t\tdrawOnCanvas(newPoints, myColorValues);\t\t\timshow(&quot;Image&quot;, img);\t\t&#125;\t&#125;\treturn 0;&#125;\n\n\n代码跑起来了，就来认真学习一下学长的 “高级” 代码呐~\n解剖代码ing~\n全局变量Mat img;vector&lt;vector&lt;int&gt;&gt; newPoints;//HSV颜色空间vector&lt;vector&lt;int&gt;&gt; myColors&#123; &#123;0,87,54,15,205,245&#125;,//橙色（hmin smin vmin hmax smax vmax）\t\t\t\t\t\t\t\t&#123;0,0,0,0,0,0&#125; &#125;;//绿色（hmin smin vmin hmax smax vmax）//RGB颜色空间vector&lt;Scalar&gt; myColorValues&#123; &#123;255,0,255&#125;,//蓝色\t\t\t\t\t\t\t\t&#123;0,255,0&#125; &#125;;//绿色\n绘制void drawOnCanvas(vector&lt;vector&lt;int&gt;&gt; newPoints, vector&lt;Scalar&gt; myColorValues) &#123;\tfor (int i = 1; i &lt; newPoints.size(); i++) &#123;\t\tline(img, Point(newPoints[i - 1][0], newPoints[i - 1][1]), Point(newPoints[i][0], newPoints[i][1]), Scalar(0, 255, 255), 10);\t&#125;&#125;//把 newPoints[i- 1] 和 newPoints[i] 连起来\nline\nvoid line(Mat&amp; img, Point pt1, Point pt2, const Scalar&amp; color, int thickness=1, int lineType=8, int shift=0)//img 要绘制线段的图像//pt1 线段的起点//pt2 线段的终点//color 线段的颜色 Scalar对象定义//thickness 线条的宽度//lineType 线段的类型 8(默认)/4/CV_AA(高斯滤波)//shift 坐标点小数点位数\n颜色过滤 &amp; 寻找对象vector&lt;vector&lt;int&gt;&gt; findColor(Mat img) &#123;\tMat imgHSV;\tcvtColor(img, imgHSV, COLOR_BGR2HSV);//转换图像到HSV空间，在其中查找颜色更加容易\tfor (int i = 0; i &lt; myColors.size(); i++)\t&#123;\t\tScalar lower(myColors[i][0], myColors[i][1], myColors[i][2]);\t\tScalar upper(myColors[i][3], myColors[i][4], myColors[i][5]);\t\tMat mask;\t\tinRange(imgHSV, lower, upper, mask);//定义颜色下限和上限，因为由于照明和不同的阴影，颜色的值将不完全相同，会是一个值的范围\t\t//imshow(to_string(i), mask);\t\tPoint myPoint = getContours(mask);\t\tif (myPoint.x != 0 &amp;&amp; myPoint.y != 0) &#123;//没检测到东西的时候就不加入新点\t\t\tnewPoints.push_back(&#123; myPoint.x,myPoint.y,i &#125;);//i为颜色索引\t\t&#125;\t&#125;\treturn newPoints;//返回检测到物体轮廓的点集&#125;\nScalar\ntypedef struct Scalar&#123;    double val[4];&#125;Scalar;\n将各个通道的值构成一个整体\n\n越看代码越觉得眼熟，欢迎回到前面的博客（bilibili教程代码——project1）\n（我说怎么写代码写这么多注释……\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"写博客","url":"/2021/01/18/%E5%86%99%E5%8D%9A%E5%AE%A2/","content":"Colin 教我写博客\n\n打广告 Colin’s Space\ncmd\n$ d:$ cd blog$ cd blog711$ hexo new posts haha$ hexo g$ hexo sctrl + cY$ hexo clean$ exit\n","categories":["HDU's Life"],"tags":["other"]},{"title":"Speech_subwayMusician","url":"/2021/07/31/Speech-subwayMusician/","content":"是演讲集训了~\n\nAt a subway station in Washington D.C., a few people stop their feet to enjoy even to listen to the music composed by the world’s top musicians during the morning rush hour. The story is kind of ridiculous but it really happens in our daily life. People get up, rush out the door and throw themselves into busy work without a second thought. People nowadays do lack of eyes to discover beauty, do lack of ears to listen to nature and do lack of heart to enjoy life itself.\nA few days ago, one of my classmates texted me telling me how anxious she was. Being a freshman learning computer science and technology, she not only become a night owl, but also need to get up early in the morning. She learns very hard but the outcome made her doubt the value of herself and the beauty of her life. I took her to a park nearby. After that she changed her attitude towards life, being delighted and hopeful.\nThe world never lack of beauty and hope. But people shut their ears, totally immerse in busy work even the work brings bitter and complain why life is so tough. People have no time stopping their feet even a little while. However there are still a group of people who devote themselves to slow down the life pace like the musician in Washington D.C..\nThe world is so beautiful. It integrates all kinds of people. Some rush forward to make the world a better place, others try to slow down their pace to make spiritual world a more peaceful place.\nDon’t stop for too long. Don’t go too fast. Open your heart, feeling the beautiful world around you. thank you.\n","categories":["HDU's Learning"],"tags":["English speech"]},{"title":"Hello World","url":"/2021/01/17/hello-world/","content":"Welcome to Hexo! \n\nThis is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\nMore info: Writing\nRun server$ hexo server\nMore info: Server\nGenerate static files$ hexo generate\nMore info: Generating\nDeploy to remote sites$ hexo deploy\nMore info: Deployment\n","tags":["other"]},{"title":"OpenCV (代码目录)","url":"/2021/08/03/codesToc/","content":"搬！\n\nChapter01\nChapter02s\nChapter03\nChapter04\nChapter05\nChapter06\nChapter07\nChapter08\nChapter09\nChapter10\nChapter11\nChapter12\nChapter13\nChapter14\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"pronounciation","url":"/2021/08/03/pronounciation/","content":"I saw the most beautiful thing ever one morning.\nIt was early with a slight frost on the ground,\nand I was walking on dewy grass down to the horse paddocks\n- my mind on the days chores.\nSuddenly a blazing laser flash erupted straight from the ground about 15 feet in front of me,\nand shot up into the sky.\nIt was so brilliant, clear and clean, it literally topped me in my tracks.\nI thought the only thing that could make such a dazzling flash must be a diamond\n- maybe someone had lost some kind of jewelry.\nOr some kind of weird reverse lightning strike\n- so bright.\nI waited to see if it repeated but it didn’t.\nI walked slowly towards it.\nThere was nothing to see.\nI stared down at the grass,\nsearching for a glint of gold or some kind of metal.\nNothing.\nSo I dropped down really close and there it was!\nA tiny - no bigger than a dime - trapdoor spider’s web,\ndelicately hanging between blades of grass,\nglistening with miniature drops of dew,\nflashing and twinkling in a myriad of colors.\nThe rising sun had caught the exact angle of the dewdrops,\nand that laser light had exploded up into the sky.\nIt was incredible and stunning,\nsuch a powerful flash from something so small and fragile,\nand I would have crushed it beneath my feet.\nIt was as though the unseen world was giving me a heads-up.\nHello! Look what’s around you.\nI’ve never forgotten that moment.\nSo I say to everyone-\ntake some time to notice the miracle of nature that most of us never even see.\nSo much beauty all around us if we would only take the time.\nLook at the brilliant colors and intricate patterns of tiny flowers that cover playing fields\n- we walk all over them without a second glance.\nWatch a bee harvesting pollen .\nSo busy with a purpose.\nTiny ants going about their day.\nBirds singing and fluffing their wings, being bossy.\nBusy iridescent beetles and glossy lizards.\nThe subtle shading and colors of practically any flower on earth are breathtaking and all natural\n- if we would only notice.\nIt doesn’t have to be a garden.\nIt can even be a weed flowering in the pavement crack.\nSo I say -\ntake the time\nand each day discover from nature one secret beautiful thing that you can keep in your heart.\nAnd get your children to do the same.\nTake a picture.\nit’s what life really consists of.\nAnd it’s free.\n","categories":["HDU's Learning"],"tags":["English speech"]},{"title":"杂想1","url":"/2021/08/04/thinking1/","content":"真实自有万钧之力\n\n\n很多事情，说起来词不达意，不说又被认为刻意隐瞒\n很多事情，我不理解，却当眼泪夺眶时逐渐明晰\n《看见》中的话 —— “真实自有万钧之力”\n很多事情，我觉得我无法理解，我觉得我无法被理解\n面对这些事情，我只想说，真实自有万钧之力\n\n以前，面对两个弟弟\n当他们犯错时，我很想纠正他们的行为\n只要他们不重蹈覆辙，我就会认为，犯错的事情过去了\n直到如今，我已经是一名大学生了\n弟弟们也长大了\n一个也读了大学，另一个则在美国求学\n很多时候，我很感恩，爸爸妈妈给了我两个弟弟\n现在也是\n弟弟们长大了，可是还是会犯错\n我也是\n可如今，当我面对犯错的他们时，我不再去纠结对错\n不是我不知道何为对、何为错，而是我发觉对于 “孩子” 来说\n有时，他们的感情远比事情的对错更为重要\n因为，当我犯错被责骂时，我心里是多么的清楚，“我错了”\n比起对我的指责，我更希望有人能关注我的感情\n犯错的孩子，心里也总是不好受的吧，尤其是明知故犯的孩子\n\n“你不必把自己装成姐姐的样子”\n“可我就是姐姐啊”\n“所以你不用装”\n\n想对妹妹说\n生活很美好\n世界不如想象般宁静，却要用心灵的平安与喜乐与世界抗衡\n\n想对所有 fall in love 的人说\n不管你该不该在你的年龄进入恋爱\n请享受纯粹的幸福，细心呵护，双向奔赴\n","categories":["HDU's Life"],"tags":["other"]},{"title":"杂想2","url":"/2021/08/07/thinking2/","content":"我将永远困惑，也永远寻找。困惑是我的诚实，寻找是我的勇敢。——周国平\n\n篇（一）那天见了她，我才对一句话有所体会\n“婚姻可以改变一个女人的一辈子”\n认识她是在好多年前，那时候的她正在闹离婚，也许是已经离了\n她有一个女儿，瘦瘦的，有点皮\n当时还是个小丫头，喜欢画画，玩儿起来比谁都疯\n回忆起那时候的她\n印象里是高梳着辫子，穿着一身紧致的衣服\n不怎么光滑的脸上涂满了”胭脂腮红“\n脸上也时常挂满笑容，却藏不住眉宇间的一抹忧伤\n她很爱小丫头，甚至有一点宠溺\n我因转学到杭州读书，便许久没有见过她\n前几天，她联系我说，近月里她待在杭州，得知我也在此，能否聚一聚\n我去了\n见面之前我对她近况的了解就是，再婚后生了一个女儿，且那个小丫头有些许抑郁\n他们是为了给小丫头看医生，才在杭州久待了些时日\n再见面时，她穿着一身暗淡无光，宽松肥大的衣服\n手里多了个两个月大的女儿，人也发福了不少\n我们一起逛了逛，然后吃了顿饭\n她变了\n蓬松的头发，肥大的衣服\n脸上除了岁月的痕迹，别无他物\n她笑起来时，连眉毛都在笑\n那个小丫头还是如以往般皮，随着年龄的增加，逐渐有了些恋财的感觉\n每当小丫头不听话时，她总是很耐心地引导小丫头\n至于那个两个月大的女儿，她笑着对我说：”她很听话，不哭也不闹“\n她总是会看看两个月大的女儿，然后嘴角扬起笑容\n我不知道我更喜欢什么时候的她\n以前的她，看起来美丽，却不那么喜悦\n如今的她，看起来苍老疲惫，却总能用眉头在笑\n也许她更享受现在点滴的幸福吧\n\n篇（二）最近总是长坐在实验室里，打着自己也不是很懂的代码\n有人问我，你喜欢学计算机吗\n我觉得，作为一个意外考入卓越学院计科英才班的我，很难回答\n我从来没有想过自己会学计算机，更没想到会在杭电学计算机\n有的时候，我也会想象，自己坐在桌子前打代码的样子\n有些欣慰，也有些遗憾\n不管怎么说，也不能说毫无兴趣，继续加油吧\n况且我还不是一个人\n\n篇（三）困惑就是\n为什么我还不会化妆，且没有动力去学\n寻找就是\n在学着打扮了\n嘻嘻嘻\n","categories":["HDU's Life"],"tags":["other"]},{"title":"无问西东","url":"/2021/07/29/%E6%97%A0%E9%97%AE%E8%A5%BF%E4%B8%9C/","content":"一路走来，一路盛开\n\n经典语录\n如果提前了解了你们要面对的人生，不知道你们是否还会有勇气前来。\n这个世界缺的不是完美的人，而是从心底给出的真心，正义，无畏与同情。\n看到和听到的，经常会令你们沮丧，世俗是这样强大，强大到生不出改变它们的念头来。可是如果有机会提前了解了你们的人生，知道青春也不过只有这些日子，不知你们是否还会在意那些世俗希望你们在意的事情。比如占有多少才更荣耀，拥有什么才能被爱。\n等你们长大，你们会因绿芽冒出土地而喜悦，会对初升的朝阳欢呼跳跃。也会给别人善意和温暖。但是却会在赞美别的生命的同时，常常，甚至永远地忘了自己的珍贵。愿你在被打击时，记起你的珍贵，抵抗恶意；愿你在迷茫时，坚信你的珍贵，爱你所爱，行你所行，听从你心，无问西东。\n什么是真实？你看到什么，听到什么，做什么，和谁在一起。有一种从心灵深处满溢出来的不懊悔，也不羞耻的和平与喜悦。\n人把自己置身于忙碌之中，有一种麻木的踏实，但丧失了真实，你的青春不过只有这些日子。\n那一刻，我从思索生命意义的羞耻感中释放出来，希望你们今后的岁月里，不要放弃对生命的思索，对自己的真实。\n世界于你而言，毫无意义和目的，却又充满随心所欲的幻想。但又有谁知，也许就在这闷热令人疲倦的正午，那个陌生人，提着满篮奇妙的货物，路过你的门前，他响亮的叫卖着，你就会从朦胧的梦中惊醒，走出房门，迎接命运的安排——《泰戈尔的诗》。\n你别怕，我就是那个给你托底的人，我会跟你一起往下掉。不管掉得有多深，我都会在下面给你托着。我最怕的是，掉的时候你把我推开，不让我给你托着。\n他们的爱与风华，只问自由，只问盛放，只问深情，只问初心，只问勇敢，无问西东。\n你怪她没有真实，你给她真实的力量了吗？\n逝者已矣，生者如斯，对以后的人好吧。\n一生太短，一瞬好长，我们哭着醒来，又哭着遗忘。\n\n观后感什么是真正的平淡？真正的平淡是指，生命中曾经历过轰轰烈烈与刻骨铭心后，才感悟到平淡的可贵。而不是一生碌碌无为，却安慰自己平凡是真。\n电影前前后后讲了四个时代的故事，四个不同时代的年轻人分别做出了自己的选择。\n吴岭澜听了泰戈尔的演讲后，遵从本心选择了文学之路；沈光耀从军牺牲；被诬陷险些丧生王敏佳、边防献身者李想、核事业贡献者陈鹏；张果果真心帮助四胞胎一家，并拒绝 robert 的提议。\n他们做出无悔选择，结局或好或坏，终究死而无憾。\n坚持初心 &amp;&amp; 传递爱\n不忘初心，方得始终，念念不忘，必有回响。\n","categories":["HDU's Life"],"tags":["movie"]},{"title":"银河补习班","url":"/2021/07/30/%E9%93%B6%E6%B2%B3%E8%A1%A5%E4%B9%A0%E7%8F%AD/","content":"白驹过隙，将梦想藏在裙子里\n\n马皓文因一次意外事故而入狱，让他遗憾地错过了儿子七年的成长时光。用自己独特的教育方法和满满的爱给予儿子马飞自由成长的空间，教会儿子独立思考的能力和面对困难的勇气。\n台词集\n\n当你能够做到自己身处黑暗之中，还能把光明留给别人，你就是一个成年人了。\n不，他人生最重要的时刻，一定是均匀散在每一天。\n人生就像射箭，梦想就像箭靶子。如果箭靶子都找不到，你每天张弓有什么意义。\n该冒的险，我是不会错过的。\n永远不要停止思考，永远不要服输。只要你不害怕，没有人，能挡住你的去向。\n\n","categories":["HDU's Life"],"tags":["movie"]},{"title":"OpenCV (计算立体图像的深度)","url":"/2021/08/04/OPENCV1-6/","content":"two camera\n\n要计算立体视觉系统的深度图，就必须计算每个像素的视差。\n\n得到水平极线用鲁棒匹配算法 (robustMatching) ，计算立体视觉系统的基础矩阵，得到水平极线。\nrobustMatcher.h\n#if !defined MATCHER#define MATCHER#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;opencv2/core.hpp&gt;#include &lt;opencv2/imgproc.hpp&gt;#include &lt;opencv2/highgui.hpp&gt;#include &lt;opencv2/features2d.hpp&gt;#include &lt;opencv2/calib3d.hpp&gt;#include &lt;opencv2/xfeatures2d.hpp&gt;#define NOCHECK      0#define CROSSCHECK   1#define RATIOCHECK   2#define BOTHCHECK    3class RobustMatcher &#123;  private:\t  // pointer to the feature point detector object\t  cv::Ptr&lt;cv::FeatureDetector&gt; detector;\t  // pointer to the feature descriptor extractor object\t  cv::Ptr&lt;cv::DescriptorExtractor&gt; descriptor;\t  int normType;\t  float ratio; // max ratio between 1st and 2nd NN\t  bool refineF; // if true will refine the F matrix\t  bool refineM; // if true will refine the matches (will refine F also)\t  double distance; // min distance to epipolar\t  double confidence; // confidence level (probability)  public:\t  RobustMatcher(const cv::Ptr&lt;cv::FeatureDetector&gt; &amp;detector, \t\t            const cv::Ptr&lt;cv::DescriptorExtractor&gt; &amp;descriptor= cv::Ptr&lt;cv::DescriptorExtractor&gt;())\t\t  : detector(detector), descriptor(descriptor),normType(cv::NORM_L2), \t\t    ratio(0.8f), refineF(true), refineM(true), confidence(0.98), distance(1.0) &#123;\t  \t\t// in this case use the associated descriptor\t\tif (!this-&gt;descriptor) &#123; \t\t\tthis-&gt;descriptor = this-&gt;detector;\t\t&#125; \t  &#125;\t  // Set the feature detector\t  void setFeatureDetector(const cv::Ptr&lt;cv::FeatureDetector&gt;&amp; detect) &#123;\t\t  this-&gt;detector= detect;\t  &#125;\t  // Set descriptor extractor\t  void setDescriptorExtractor(const cv::Ptr&lt;cv::DescriptorExtractor&gt;&amp; desc) &#123;\t\t  this-&gt;descriptor= desc;\t  &#125;\t  // Set the norm to be used for matching\t  void setNormType(int norm) &#123;\t\t  normType= norm;\t  &#125;\t  // Set the minimum distance to epipolar in RANSAC\t  void setMinDistanceToEpipolar(double d) &#123;\t\t  distance= d;\t  &#125;\t  // Set confidence level in RANSAC\t  void setConfidenceLevel(double c) &#123;\t\t  confidence= c;\t  &#125;\t  // Set the NN ratio\t  void setRatio(float r) &#123;\t\t  ratio= r;\t  &#125;\t  // if you want the F matrix to be recalculated\t  void refineFundamental(bool flag) &#123;\t\t  refineF= flag;\t  &#125;\t  // if you want the matches to be refined using F\t  void refineMatches(bool flag) &#123;\t\t  refineM= flag;\t  &#125;\t  // Clear matches for which NN ratio is &gt; than threshold\t  // return the number of removed points \t  // (corresponding entries being cleared, i.e. size will be 0)      int ratioTest(const std::vector&lt;std::vector&lt;cv::DMatch&gt; &gt;&amp; inputMatches,\t\t            std::vector&lt;cv::DMatch&gt;&amp; outputMatches) &#123;\t\tint removed=0;        // for all matches        for (std::vector&lt;std::vector&lt;cv::DMatch&gt; &gt;::const_iterator matchIterator= inputMatches.begin();\t\t\t matchIterator!= inputMatches.end(); ++matchIterator) &#123;\t\t\t\t \t\t\t\t //   first best match/second best match\t\t\t\t if ((matchIterator-&gt;size() &gt; 1) &amp;&amp; // if 2 NN has been identified \t\t\t\t\t (*matchIterator)[0].distance/(*matchIterator)[1].distance &lt; ratio) &#123;\t\t\t\t\t\t\t\t // it is an acceptable match\t\t\t\t\t outputMatches.push_back((*matchIterator)[0]);\t\t\t\t &#125; else &#123;\t\t\t\t\t removed++;\t\t\t\t &#125;\t\t&#125;\t\treturn removed;\t  &#125;\t  // Insert symmetrical matches in symMatches vector\t  void symmetryTest(const std::vector&lt;cv::DMatch&gt;&amp; matches1,\t\t                const std::vector&lt;cv::DMatch&gt;&amp; matches2,\t\t\t\t\t    std::vector&lt;cv::DMatch&gt;&amp; symMatches) &#123;\t\t\t\t\t// for all matches image 1 -&gt; image 2\t\tfor (std::vector&lt;cv::DMatch&gt;::const_iterator matchIterator1= matches1.begin();\t\t\t matchIterator1!= matches1.end(); ++matchIterator1) &#123;\t\t\t// for all matches image 2 -&gt; image 1\t\t\tfor (std::vector&lt;cv::DMatch&gt;::const_iterator matchIterator2= matches2.begin();\t\t\t\tmatchIterator2!= matches2.end(); ++matchIterator2) &#123;\t\t\t\t// Match symmetry test\t\t\t\tif (matchIterator1-&gt;queryIdx == matchIterator2-&gt;trainIdx  &amp;&amp; \t\t\t\t\tmatchIterator2-&gt;queryIdx == matchIterator1-&gt;trainIdx) &#123;\t\t\t\t\t\t// add symmetrical match\t\t\t\t\t\tsymMatches.push_back(*matchIterator1);\t\t\t\t\t\tbreak; // next match in image 1 -&gt; image 2\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;\t  &#125;\t  // Apply both ratio and symmetry test\t  // (often an over-kill)      void ratioAndSymmetryTest(const std::vector&lt;std::vector&lt;cv::DMatch&gt; &gt;&amp; matches1,                                const std::vector&lt;std::vector&lt;cv::DMatch&gt; &gt;&amp; matches2,\t\t\t\t\t            std::vector&lt;cv::DMatch&gt;&amp; outputMatches) &#123;\t\t// Remove matches for which NN ratio is &gt; than threshold\t\t// clean image 1 -&gt; image 2 matches\t\tstd::vector&lt;cv::DMatch&gt; ratioMatches1;\t\tint removed= ratioTest(matches1,ratioMatches1);\t\tstd::cout &lt;&lt; &quot;Number of matched points 1-&gt;2 (ratio test) : &quot; &lt;&lt; ratioMatches1.size() &lt;&lt; std::endl;\t\t// clean image 2 -&gt; image 1 matches\t\tstd::vector&lt;cv::DMatch&gt; ratioMatches2;\t\tremoved= ratioTest(matches2,ratioMatches2);\t\tstd::cout &lt;&lt; &quot;Number of matched points 1-&gt;2 (ratio test) : &quot; &lt;&lt; ratioMatches2.size() &lt;&lt; std::endl;\t\t// Remove non-symmetrical matches\t\tsymmetryTest(ratioMatches1,ratioMatches2,outputMatches);\t\tstd::cout &lt;&lt; &quot;Number of matched points (symmetry test): &quot; &lt;&lt; outputMatches.size() &lt;&lt; std::endl;\t  &#125;\t  // Identify good matches using RANSAC\t  // Return fundamental matrix and output matches\t  cv::Mat ransacTest(const std::vector&lt;cv::DMatch&gt;&amp; matches,\t\t                 std::vector&lt;cv::KeyPoint&gt;&amp; keypoints1, \t\t\t\t\t\t std::vector&lt;cv::KeyPoint&gt;&amp; keypoints2,\t\t\t\t\t     std::vector&lt;cv::DMatch&gt;&amp; outMatches) &#123;\t\t// Convert keypoints into Point2f\t\t\tstd::vector&lt;cv::Point2f&gt; points1, points2;\t\t\tfor (std::vector&lt;cv::DMatch&gt;::const_iterator it= matches.begin();\t\t\t it!= matches.end(); ++it) &#123;\t\t\t // Get the position of left keypoints\t\t\t points1.push_back(keypoints1[it-&gt;queryIdx].pt);\t\t\t // Get the position of right keypoints\t\t\t points2.push_back(keypoints2[it-&gt;trainIdx].pt);\t    &#125;\t\t// Compute F matrix using RANSAC\t\tstd::vector&lt;uchar&gt; inliers(points1.size(),0);\t\tcv::Mat fundamental= cv::findFundamentalMat(\t\t\tpoints1,points2, // matching points\t\t    inliers,         // match status (inlier or outlier)  \t\t    cv::FM_RANSAC,   // RANSAC method\t\t    distance,        // distance to epipolar line\t\t    confidence);     // confidence probability\t\t\t// extract the surviving (inliers) matches\t\tstd::vector&lt;uchar&gt;::const_iterator itIn= inliers.begin();\t\tstd::vector&lt;cv::DMatch&gt;::const_iterator itM= matches.begin();\t\t// for all matches\t\tfor ( ;itIn!= inliers.end(); ++itIn, ++itM) &#123;\t\t\tif (*itIn) &#123; // it is a valid match\t\t\t\toutMatches.push_back(*itM);\t\t\t&#125;\t\t&#125;\t\tif (refineF || refineM) &#123;\t\t// The F matrix will be recomputed with all accepted matches\t\t\t// Convert keypoints into Point2f for final F computation\t\t\t\tpoints1.clear();\t\t\tpoints2.clear();\t\t\t\tfor (std::vector&lt;cv::DMatch&gt;::const_iterator it= outMatches.begin();\t\t\t\t it!= outMatches.end(); ++it) &#123;\t\t\t\t // Get the position of left keypoints\t\t\t\t points1.push_back(keypoints1[it-&gt;queryIdx].pt);\t\t\t\t // Get the position of right keypoints\t\t\t\t points2.push_back(keypoints2[it-&gt;trainIdx].pt);\t\t\t&#125;\t\t\t// Compute 8-point F from all accepted matches\t\t\tfundamental= cv::findFundamentalMat(\t\t\t\tpoints1,points2, // matching points\t\t\t\tcv::FM_8POINT); // 8-point method\t\t\tif (refineM) &#123;\t\t\t\tstd::vector&lt;cv::Point2f&gt; newPoints1, newPoints2;\t\t\t\t\t// refine the matches\t\t\t\tcorrectMatches(fundamental,             // F matrix\t\t\t\t\t           points1, points2,        // original position\t\t\t\t\t\t\t   newPoints1, newPoints2); // new position\t\t\t\tfor (int i=0; i&lt; points1.size(); i++) &#123;\t\t\t\t\tstd::cout &lt;&lt; &quot;(&quot; &lt;&lt; keypoints1[outMatches[i].queryIdx].pt.x \t\t\t\t\t\t      &lt;&lt; &quot;,&quot; &lt;&lt; keypoints1[outMatches[i].queryIdx].pt.y \t\t\t\t\t\t\t  &lt;&lt; &quot;) -&gt; &quot;;\t\t\t\t\tstd::cout &lt;&lt; &quot;(&quot; &lt;&lt; newPoints1[i].x \t\t\t\t\t\t      &lt;&lt; &quot;,&quot; &lt;&lt; newPoints1[i].y &lt;&lt; std::endl;\t\t\t\t\tstd::cout &lt;&lt; &quot;(&quot; &lt;&lt; keypoints2[outMatches[i].trainIdx].pt.x \t\t\t\t\t\t      &lt;&lt; &quot;,&quot; &lt;&lt; keypoints2[outMatches[i].trainIdx].pt.y \t\t\t\t\t\t\t  &lt;&lt; &quot;) -&gt; &quot;;\t\t\t\t\tstd::cout &lt;&lt; &quot;(&quot; &lt;&lt; newPoints2[i].x \t\t\t\t\t\t      &lt;&lt; &quot;,&quot; &lt;&lt; newPoints2[i].y &lt;&lt; std::endl;\t\t\t\t\tkeypoints1[outMatches[i].queryIdx].pt.x= newPoints1[i].x;\t\t\t\t\tkeypoints1[outMatches[i].queryIdx].pt.y= newPoints1[i].y;\t\t\t\t\tkeypoints2[outMatches[i].trainIdx].pt.x= newPoints2[i].x;\t\t\t\t\tkeypoints2[outMatches[i].trainIdx].pt.y= newPoints2[i].y;\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;\t\treturn fundamental;\t  &#125;\t  // Match feature points using RANSAC\t  // returns fundamental matrix and output match set\t  cv::Mat match(cv::Mat&amp; image1, cv::Mat&amp; image2, // input images \t\t  std::vector&lt;cv::DMatch&gt;&amp; matches, // output matches and keypoints\t\t  std::vector&lt;cv::KeyPoint&gt;&amp; keypoints1, std::vector&lt;cv::KeyPoint&gt;&amp; keypoints2,\t\t  int check=CROSSCHECK) &#123;  // check type (symmetry or ratio or none or both)\t\t// 1. Detection of the feature points\t\tdetector-&gt;detect(image1,keypoints1);\t\tdetector-&gt;detect(image2,keypoints2);\t\tstd::cout &lt;&lt; &quot;Number of feature points (1): &quot; &lt;&lt; keypoints1.size() &lt;&lt; std::endl;\t\tstd::cout &lt;&lt; &quot;Number of feature points (2): &quot; &lt;&lt; keypoints2.size() &lt;&lt; std::endl;\t\t// 2. Extraction of the feature descriptors\t\tcv::Mat descriptors1, descriptors2;\t\tdescriptor-&gt;compute(image1,keypoints1,descriptors1);\t\tdescriptor-&gt;compute(image2,keypoints2,descriptors2);\t\tstd::cout &lt;&lt; &quot;descriptor matrix size: &quot; &lt;&lt; descriptors1.rows &lt;&lt; &quot; by &quot; &lt;&lt; descriptors1.cols &lt;&lt; std::endl;\t\t// 3. Match the two image descriptors\t\t//    (optionaly apply some checking method)   \t\t// Construction of the matcher with crosscheck \t\tcv::BFMatcher matcher(normType,            //distance measure\t                          check==CROSSCHECK);  // crosscheck flag                             \t\t// vectors of matches        std::vector&lt;std::vector&lt;cv::DMatch&gt; &gt; matches1;        std::vector&lt;std::vector&lt;cv::DMatch&gt; &gt; matches2;\t    std::vector&lt;cv::DMatch&gt; outputMatches;\t\t// call knnMatch if ratio check is required\t\tif (check==RATIOCHECK || check==BOTHCHECK) &#123;\t\t\t// from image 1 to image 2\t\t\t// based on k nearest neighbours (with k=2)\t\t\tmatcher.knnMatch(descriptors1,descriptors2, \t\t\t\tmatches1, // vector of matches (up to 2 per entry) \t\t\t\t2);\t\t  // return 2 nearest neighbours\t\t\tstd::cout &lt;&lt; &quot;Number of matched points 1-&gt;2: &quot; &lt;&lt; matches1.size() &lt;&lt; std::endl;\t\t\tif (check==BOTHCHECK) &#123;\t\t\t\t// from image 2 to image 1\t\t\t\t// based on k nearest neighbours (with k=2)\t\t\t\tmatcher.knnMatch(descriptors2,descriptors1, \t\t\t\t\tmatches2, // vector of matches (up to 2 per entry) \t\t\t\t\t2);\t\t  // return 2 nearest neighbours\t\t\t\tstd::cout &lt;&lt; &quot;Number of matched points 2-&gt;1: &quot; &lt;&lt; matches2.size() &lt;&lt; std::endl;\t\t\t&#125;\t\t&#125; \t\t\t\t// select check method\t\tswitch (check) &#123;\t\t\tcase CROSSCHECK:\t\t\t\tmatcher.match(descriptors1,descriptors2,outputMatches);\t\t\t\tstd::cout &lt;&lt; &quot;Number of matched points 1-&gt;2 (after cross-check): &quot; &lt;&lt; outputMatches.size() &lt;&lt; std::endl;\t\t\t\tbreak;\t\t\tcase RATIOCHECK:\t\t\t\tratioTest(matches1,outputMatches);\t\t\t\tstd::cout &lt;&lt; &quot;Number of matched points 1-&gt;2 (after ratio test): &quot; &lt;&lt; outputMatches.size() &lt;&lt; std::endl;\t\t\t\tbreak;\t\t\tcase BOTHCHECK:\t\t\t\tratioAndSymmetryTest(matches1,matches2,outputMatches);\t\t\t\tstd::cout &lt;&lt; &quot;Number of matched points 1-&gt;2 (after ratio and cross-check): &quot; &lt;&lt; outputMatches.size() &lt;&lt; std::endl;\t\t\t\tbreak;\t\t\tcase NOCHECK:\t\t\tdefault:\t\t\t\tmatcher.match(descriptors1,descriptors2,outputMatches);\t\t\t\tstd::cout &lt;&lt; &quot;Number of matched points 1-&gt;2: &quot; &lt;&lt; outputMatches.size() &lt;&lt; std::endl;\t\t\t\tbreak;\t\t&#125;\t\t// 4. Validate matches using RANSAC\t\tcv::Mat fundamental= ransacTest(outputMatches, keypoints1, keypoints2, matches);\t\tstd::cout &lt;&lt; &quot;Number of matched points (after RANSAC): &quot; &lt;&lt; matches.size() &lt;&lt; std::endl;\t\t// return the found fundamental matrix\t\treturn fundamental;\t&#125;\t  \t // Match feature points using RANSAC\t // returns fundamental matrix and output match set     // this is the simplified version presented in the book\t  cv::Mat matchBook(cv::Mat&amp; image1, cv::Mat&amp; image2, // input images \t\t  std::vector&lt;cv::DMatch&gt;&amp; matches, // output matches and keypoints\t\t  std::vector&lt;cv::KeyPoint&gt;&amp; keypoints1, std::vector&lt;cv::KeyPoint&gt;&amp; keypoints2) &#123; \t\t\t  \t\t// 1. Detection of the feature points\t\tdetector-&gt;detect(image1,keypoints1);\t\tdetector-&gt;detect(image2,keypoints2);\t\t// 2. Extraction of the feature descriptors\t\tcv::Mat descriptors1, descriptors2;\t\tdescriptor-&gt;compute(image1,keypoints1,descriptors1);\t\tdescriptor-&gt;compute(image2,keypoints2,descriptors2);\t\t// 3. Match the two image descriptors\t\t//    (optionnally apply some checking method)   \t\t// Construction of the matcher with crosscheck \t\tcv::BFMatcher matcher(normType,   //distance measure\t                          true);      // crosscheck flag                             \t\t// match descriptors\t    std::vector&lt;cv::DMatch&gt; outputMatches;\t\tmatcher.match(descriptors1,descriptors2,outputMatches);\t\t// 4. Validate matches using RANSAC\t\tcv::Mat fundamental= ransacTest(outputMatches, keypoints1, keypoints2, matches);\t\t// return the found fundemental matrix\t\treturn fundamental;\t&#125;&#125;;#endif\nrobustMatcher.cpp\n#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;numeric&gt;#include &lt;opencv2/core.hpp&gt;#include &lt;opencv2/imgproc.hpp&gt;#include &lt;opencv2/highgui.hpp&gt;#include &lt;opencv2/features2d.hpp&gt;#include &lt;opencv2/calib3d.hpp&gt;#include &lt;opencv2/objdetect.hpp&gt;#include &lt;opencv2/xfeatures2d.hpp&gt;#include &lt;opencv2/viz.hpp&gt;#include &quot;robustMatcher.h&quot;int main()&#123;\t// Read input images\tcv::Mat image1= cv::imread(&quot;brebeuf1.jpg&quot;,0);\tcv::Mat image2= cv::imread(&quot;brebeuf2.jpg&quot;,0);\tif (!image1.data || !image2.data)\t\treturn 0; \t// Prepare the matcher (with default parameters)\t// here SIFT detector and descriptor\tRobustMatcher rmatcher(cv::xfeatures2d::SIFT::create(250));\t// Match the two images\tstd::vector&lt;cv::DMatch&gt; matches;\tstd::vector&lt;cv::KeyPoint&gt; keypoints1, keypoints2;\tcv::Mat fundamental = rmatcher.match(image1, image2, matches,\t\tkeypoints1, keypoints2);\t// draw the matches\tcv::Mat imageMatches;\tcv::drawMatches(image1, keypoints1,  // 1st image and its keypoints\t\timage2, keypoints2,  // 2nd image and its keypoints\t\tmatches,\t\t\t// the matches\t\timageMatches,\t\t// the image produced\t\tcv::Scalar(255, 255, 255),  // color of the lines\t\tcv::Scalar(255, 255, 255),  // color of the keypoints\t\tstd::vector&lt;char&gt;(),\t\t2);\tcv::namedWindow(&quot;Matches&quot;);\tcv::imshow(&quot;Matches&quot;, imageMatches);\t// Convert keypoints into Point2f\t\tstd::vector&lt;cv::Point2f&gt; points1, points2;\tfor (std::vector&lt;cv::DMatch&gt;::const_iterator it = matches.begin();\tit != matches.end(); ++it) &#123;\t\t// Get the position of left keypoints\t\tfloat x = keypoints1[it-&gt;queryIdx].pt.x;\t\tfloat y = keypoints1[it-&gt;queryIdx].pt.y;\t\tpoints1.push_back(keypoints1[it-&gt;queryIdx].pt);\t\t// Get the position of right keypoints\t\tx = keypoints2[it-&gt;trainIdx].pt.x;\t\ty = keypoints2[it-&gt;trainIdx].pt.y;\t\tpoints2.push_back(keypoints2[it-&gt;trainIdx].pt);\t&#125;\t// Compute homographic rectification\tcv::Mat h1, h2;\tcv::stereoRectifyUncalibrated(points1, points2, fundamental, image1.size(), h1, h2);\t// Rectify the images through warping\tcv::Mat rectified1;\tcv::warpPerspective(image1, rectified1, h1, image1.size());\tcv::Mat rectified2;\tcv::warpPerspective(image2, rectified2, h2, image1.size());\t// Display the images\tcv::namedWindow(&quot;Left Rectified Image&quot;);\tcv::imshow(&quot;Left Rectified Image&quot;, rectified1);\tcv::namedWindow(&quot;Right Rectified Image&quot;);\tcv::imshow(&quot;Right Rectified Image&quot;, rectified2);\tpoints1.clear();\tpoints2.clear();\tfor (int i = 20; i &lt; image1.rows - 20; i += 20) &#123;\t\tpoints1.push_back(cv::Point(image1.cols / 2, i));\t\tpoints2.push_back(cv::Point(image2.cols / 2, i));\t&#125;\t// Draw the epipolar lines\tstd::vector&lt;cv::Vec3f&gt; lines1;\tcv::computeCorrespondEpilines(points1, 1, fundamental, lines1);\tfor (std::vector&lt;cv::Vec3f&gt;::const_iterator it = lines1.begin();\tit != lines1.end(); ++it) &#123;\t\tcv::line(image2, cv::Point(0, -(*it)[2] / (*it)[1]),\t\t\tcv::Point(image2.cols, -((*it)[2] + (*it)[0] * image2.cols) / (*it)[1]),\t\t\tcv::Scalar(255, 255, 255));\t&#125;\tstd::vector&lt;cv::Vec3f&gt; lines2;\tcv::computeCorrespondEpilines(points2, 2, fundamental, lines2);\tfor (std::vector&lt;cv::Vec3f&gt;::const_iterator it = lines2.begin();\tit != lines2.end(); ++it) &#123;\t\tcv::line(image1, cv::Point(0, -(*it)[2] / (*it)[1]),\t\t\tcv::Point(image1.cols, -((*it)[2] + (*it)[0] * image1.cols) / (*it)[1]),\t\t\tcv::Scalar(255, 255, 255));\t&#125;\t// Display the images with epipolar lines\tcv::namedWindow(&quot;Left Epilines&quot;);\tcv::imshow(&quot;Left Epilines&quot;, image1);\tcv::namedWindow(&quot;Right Epilines&quot;);\tcv::imshow(&quot;Right Epilines&quot;, image2);\t// draw the pair\tcv::drawMatches(image1, keypoints1,  // 1st image \t\timage2, keypoints2,              // 2nd image \t\tstd::vector&lt;cv::DMatch&gt;(),\t\t\t\t\timageMatches,\t\t             // the image produced\t\tcv::Scalar(255, 255, 255),  \t\tcv::Scalar(255, 255, 255),  \t\tstd::vector&lt;char&gt;(),\t\t2);\tcv::namedWindow(&quot;A Stereo pair&quot;);\tcv::imshow(&quot;A Stereo pair&quot;, imageMatches);\t// Compute disparity\tcv::Mat disparity;\tcv::Ptr&lt;cv::StereoMatcher&gt; pStereo = cv::StereoSGBM::create(0,   // minimum disparity\t\t                                                        32,  // maximum disparity\t\t                                                        5);  // block size\tpStereo-&gt;compute(rectified1, rectified2, disparity);\t// draw the rectified pair\t/*\tcv::warpPerspective(image1, rectified1, h1, image1.size());\tcv::warpPerspective(image2, rectified2, h2, image1.size());\tcv::drawMatches(rectified1, keypoints1,  // 1st image \t\trectified2, keypoints2,              // 2nd image\t\tstd::vector&lt;cv::DMatch&gt;(),\t\t\t\timageMatches,\t\t                // the image produced\t\tcv::Scalar(255, 255, 255),  \t\tcv::Scalar(255, 255, 255),  \t\tstd::vector&lt;char&gt;(),\t\t2);\tcv::namedWindow(&quot;Rectified Stereo pair&quot;);\tcv::imshow(&quot;Rectified Stereo pair&quot;, imageMatches);\t*/\tdouble minv, maxv;\tdisparity = disparity * 64;\tcv::minMaxLoc(disparity, &amp;minv, &amp;maxv);\tstd::cout &lt;&lt; minv &lt;&lt; &quot;+&quot; &lt;&lt; maxv &lt;&lt; std::endl;\t// Display the disparity map\tcv::namedWindow(&quot;Disparity Map&quot;);\tcv::imshow(&quot;Disparity Map&quot;, disparity);\tcv::waitKey();\treturn 0;&#125;\n\n利用单应变换将每个相机的图像平面投影到完全对齐的虚拟平面上。\n// 计算单应变换矫正量Mat h1, h2;stereoRectifyUncalibrated(points1, points2, fundamental, image1.size(), h1, h2);// 用变换实现图像校正Mat rectified1;warpPerspective(image1, rectified1, h1, image1.size());Mat rectified2;warpPerspective(image2, rectified2, h2, image1.size()); // ??? image1 or image2// 计算视差Mat disparity;Ptr&lt;StereoMatcher&gt; pStereo = StereoSGBM::create(0, 32, 5); // 最小视差，最大视差，块的大小pStereo -&gt; compute(rectified1, rectified2, disparity);\n部分对极线\n\n经矫正的图像对\n\n视差图：亮的地方视差大，离物体近\n\n\n\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"OpenCV (bilibili教程代码)","url":"/2021/07/26/OPENCV1-1/","content":"我是代码搬运机\n\ngithub.com\nChapter 1#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Images/// &lt;/summary&gt;//void main() &#123;//\tstring path = &quot;Resources/test.png&quot;;//\tMat img=imread(path);//matrix data type 由opencv引入来处理图像//\timshow(&quot;Image&quot;, img);//\twaitKey(0);//增加延时，0表示无穷//&#125;/// &lt;summary&gt;/// Video/// //视频是一系列图像，需要遍历所有图像或帧 一一捕获并显示，因此将使用while循环/// &lt;/summary&gt;//void main() &#123;//\tstring path = &quot;Resources/test_video.mp4&quot;;//\tVideoCapture cap(path);//\tMat img;////\twhile (true) &#123;//\t\tcap.read(img);//\t\timshow(&quot;Image&quot;, img);//\t\twaitKey(20);//增加延时 20ms//\t&#125;//\t//&#125;/// &lt;summary&gt;/// Webcam/// 与导入视频不同的是，不需要视频路径，只需要给相机ID，id=0表示默认的摄像头/// &lt;/summary&gt;//void main() &#123;//\tVideoCapture cap(0);//相机id=0//\tMat img;//\t//while (true) &#123;\t//\tcap.read(img);\t//\timshow(&quot;Image&quot;, img);\t//\twaitKey(1);//增加延时 1ms，以免太慢\t//&#125;////&#125;\nChapter 2#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Basic Function/// &lt;/summary&gt;//void main() &#123;//\tstring path = &quot;Resources/test.png&quot;;//\tMat img=imread(path);//matrix data type 由opencv引入来处理图像//\tMat imgGray,imgBlur,imgCanny,imgDil,imgErode;////\tcvtColor(img, imgGray, COLOR_BGR2GRAY);//cvt是convert的缩写，将图像从一种颜色空间转换为另一种颜色空间。//\tGaussianBlur(imgGray, imgBlur,Size(7,7),5,0);//使用高斯滤波器模糊图像。该函数将源图像与指定的高斯核进行卷积,Size(7,7)是核大小,数字越大越模糊//\tCanny(imgBlur, imgCanny, 25, 75);//边缘检测，阈值1，2可调，目的：显示更多的边缘//\t//\tMat kernel = getStructuringElement(MORPH_RECT, Size(3, 3));//创建一个核，增加Size（只能是奇数）会扩张/侵蚀更多//\tdilate(imgCanny, imgDil, kernel);//扩张边缘（增加边缘厚度）//\terode(imgDil, imgErode, kernel);//侵蚀边缘（减小边缘厚度）//\timshow(&quot;Image&quot;, img);//\timshow(&quot;Image Gray&quot;, imgGray);//\timshow(&quot;Image Blur&quot;, imgBlur);//\timshow(&quot;Image Canny&quot;, imgCanny);//\timshow(&quot;Image Dilation&quot;, imgDil);//\timshow(&quot;Image Erode&quot;, imgErode);//\twaitKey(0);//增加延时，0表示无穷//&#125;\nChapter 3//学习如何调整大小以及裁剪图像#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Resize and Crop/// &lt;/summary&gt;//void main() &#123;//\tstring path = &quot;Resources/test.png&quot;;//\tMat img=imread(path);//matrix data type 由opencv引入来处理图像//\tMat imgResize,imgCrop;//\tcout &lt;&lt; img.size() &lt;&lt; endl;//打印图像尺寸//\t//resize(img, imgResize, Size(640, 480));//指定图片尺寸缩放//\tresize(img, imgResize, Size(),0.5,0.5);//指定缩放比例，不指定图片尺寸////\t//矩形数据类型//\tRect roi(200, 100, 300, 300);//以左上角为坐标原点，（200，100）为矩形的左上角坐标，300,300为矩形长宽//\timgCrop = img(roi);//裁剪图像，为了找到特定区域 添加更多处理 roi:region of interest//\timshow(&quot;Image&quot;, img);//\timshow(&quot;Image Resize&quot;, imgResize);//\timshow(&quot;Image Crop&quot;, imgCrop);//\t//\twaitKey(0);//增加延时，0表示无穷//&#125;\nChapter 4//学习如何绘制形状（圆形、矩形、线段）和如何在图片上写字#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Draw Shapes and Text/// &lt;/summary&gt;//void main() &#123;//\t//创建空白图像//\t//Scalar(255, 0, 255)，（蓝色：255，0，0），（紫色：255，0，255），（黑色：0，0，0），（白色：255，255，255）//\tMat img(512, 512, CV_8UC3, Scalar(255, 255, 255));//(512,512)为图片大小，CV8UC3中8表示每个像素的值从0到255，3表示3个颜色通道BGR,Scalar(255, 0, 0)表示图像将具有的颜色//\t//\tcircle(img, Point(256, 256), 155,Scalar(0,69,255),FILLED);//第一个参数：图片，第二个参数是圆心坐标，第三个参数是圆大小，第四个参数是颜色，第五个参数是厚度（可以不写），想要填充可以填FILLED//\trectangle(img, Point(130,226), Point(382,286),Scalar(255,255,255),FILLED);//第一个Point给矩形左上角坐标，第二个Point给矩形右下角坐标//\tline(img, Point(130, 296), Point(382, 296), Scalar(255, 255, 255), 2);//第一个Point是起点坐标、第二个Point是终点坐标//\tputText(img, &quot;zhuhuijin&quot;, Point(137, 262), FONT_HERSHEY_DUPLEX, 0.75, Scalar(0, 69, 255), 2);//\t//\timshow(&quot;Image&quot;, img);//\twaitKey(0);//增加延时，0表示无穷//&#125;\nChapter 5//学习如何扭曲图像，来扫描文档#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Warp Images/// &lt;/summary&gt;//float w = 250, h = 350;//图片大小//Mat matrix, imgWarp;//void main() &#123;//\t//图片用画图打开，在屏幕左下角会显示点的坐标//\tstring path = &quot;Resources/cards.jpg&quot;;//\tMat img=imread(path);//matrix data type 由opencv引入来处理图像//\tPoint2f src[4] = &#123; &#123;529,142&#125;,&#123;771,190&#125;,&#123;405,395&#125;,&#123;674,457&#125; &#125;;//Point2f表示浮点数//\tPoint2f dst[4] = &#123; &#123;0.0f,0.0f&#125;,&#123;w,0.0f&#125;,&#123;0.0f,h&#125;,&#123;w,h&#125; &#125;;//Point2f表示浮点数////\tmatrix = getPerspectiveTransform(src, dst);//\twarpPerspective(img, imgWarp, matrix, Point(w,h));//\t//\t//确定src坐标是否正确//\tfor (int i = 0; i &lt; 4; i++) &#123;//\t\tcircle(img, src[i], 10, Scalar(0, 0, 255), FILLED);//\t&#125;////\timshow(&quot;Image&quot;, img);//\timshow(&quot;Image Warp&quot;, imgWarp);//\twaitKey(0);//增加延时，0表示无穷//&#125;\nChapter 6//学习检测图片中的颜色，来创建特定对象的对象检测器#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Color Detection/// &lt;/summary&gt;//int hmin = 0, smin = 0, vmin = 0;//int hmax = 179, smax = 255, vmax = 255;//如何确定这6个值，每次都更改所有这些再次运行很痛苦 --&gt;创建跟踪栏（使我们可以实时更改这些值）//void main() &#123;//\tstring path = &quot;Resources/shapes.png&quot;;//\tMat img=imread(path);//\tMat imgHSV,mask;//\tcvtColor(img, imgHSV, COLOR_BGR2HSV);//转换图像到HSV空间，在其中查找颜色更加容易//\tnamedWindow(&quot;Trackbars&quot;, (640, 200));//(640,200)是尺寸//\t//运行时，把3个min的都移到最小值，把3个max的都移到最大值，然后移动使其保持为白色//\tcreateTrackbar(&quot;Hue Min&quot;, &quot;Trackbars&quot;, &amp;hmin, 179);//对于hue色相饱和度最大180,对于另外两个色相饱和度最大255//\tcreateTrackbar(&quot;Hue Max&quot;, &quot;Trackbars&quot;, &amp;hmax, 179);//\tcreateTrackbar(&quot;Sat Min&quot;, &quot;Trackbars&quot;, &amp;smin, 255);//\tcreateTrackbar(&quot;Sat Max&quot;, &quot;Trackbars&quot;, &amp;smax, 255);//\tcreateTrackbar(&quot;Val Min&quot;, &quot;Trackbars&quot;, &amp;vmin, 255);//\tcreateTrackbar(&quot;Val Max&quot;, &quot;Trackbars&quot;, &amp;vmax, 255);//\t//\twhile (true) &#123;//\t\t//检查数组元素是否位于其他两个数组的元素之间。//\t\t//imgHSV为输入图像，mask为输出图像////\t\tScalar lower(hmin, smin, vmin);//\t\tScalar upper(hmax, smax, vmax);//\t\tinRange(imgHSV, lower, upper, mask);//定义颜色下限和上限，因为由于照明和不同的阴影，颜色的值将不完全相同，会是一个值的范围//\t\timshow(&quot;Image&quot;, img);//\t\timshow(&quot;Image HSV&quot;, imgHSV);//\t\timshow(&quot;Image mask&quot;, mask);//\t\twaitKey(1);//增加延时//\t&#125;//&#125;\nChapter 7//学习如何检测形状或图像中的轮廓#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Shapes/// &lt;/summary&gt;//! 获取轮廓void getContours(Mat imgDil,Mat img) &#123;//imgDil是传入的扩张边缘的图像用来查找轮廓，img是要在其上绘制轮廓的图像\tvector&lt;vector&lt;Point&gt;&gt; contours;//轮廓检测到的轮廓。每个轮廓线存储为一个点的向量\t\tvector&lt;Vec4i&gt; hierarchy;//包含关于映像拓扑的信息  typedef Vec&lt;int, 4&gt; Vec4i;具有4个整数值\t\t//在二值图像中查找轮廓。该函数利用该算法从二值图像中提取轮廓\tfindContours(imgDil, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);\t//drawContours(img, contours, -1, Scalar(255, 0, 255), 2);//img：要绘制轮廓在什么图片上，contours：要绘制的轮廓，-1定义要绘制的轮廓号（-1表示所有轮廓），Saclar表示轮廓颜色，2表示厚度\tvector&lt;vector&lt;Point&gt;&gt; conPoly(contours.size());//conploy的数量应小于contours\tvector&lt;Rect&gt; boundRect(contours.size());\t//过滤器：通过轮廓面积来过滤噪声\tfor (int i = 0; i &lt; contours.size(); i++) &#123;//遍历检测到的轮廓\t\tint area = contourArea(contours[i]);\t\t\t\t//cout &lt;&lt; area &lt;&lt; endl;\t\t\t\tstring objectType;\t\tif (area &gt; 1000) &#123;//轮廓面积＞1000才绘制\t\t\t//计算轮廓周长或凹坑长度。该函数计算了曲线长度和封闭的周长。\t\t\tfloat peri = arcLength(contours[i], true);//计算封闭轮廓周长\t\t\tapproxPolyDP(contours[i], conPoly[i],0.02*peri,true);//以指定的精度近似多边形曲线。第二个参数conPloy[i]存储近似的结果，是输出。\t\t\t\t\t\t\t\t\tboundRect[i]=boundingRect(conPoly[i]);//计算边界矩形\t\t\t\t\t\tint objCor = (int)conPoly[i].size();//找近似多边形的角点,三角形有3个角点，矩形/正方形有4个角点，圆形&gt;4个角点\t\t\tcout &lt;&lt; objCor &lt;&lt; endl;\t\t\tif (objCor == 3) &#123;objectType = &quot;Tri&quot;;&#125;\t\t\telse if (objCor == 4) &#123;\t\t\t\tfloat aspRatio = (float)boundRect[i].width / (float)boundRect[i].height;//宽高比\t\t\t\tif (aspRatio &gt; 0.95 &amp;&amp; aspRatio &lt; 1.05) &#123; objectType = &quot;Square&quot;;&#125;//矩形的宽高比不会正好等于1\t\t\t\telse objectType = &quot;Rect&quot;;\t\t\t&#125;\t\t\telse if (objCor &gt; 4) &#123; objectType = &quot;Circle&quot;;&#125;\t\t\t\t\t\tdrawContours(img, conPoly, i, Scalar(255, 0, 255), 2);\t\t\trectangle/*绘制边界矩形*/(img, boundRect[i].tl()/*tl()：topleft矩形左上角坐标*/, boundRect[i].br()/*br()：bottom right矩形右下角坐标*/, Scalar(0, 255, 0), 5);\t\t\tputText(img, objectType, &#123;boundRect[i].x,boundRect[i].y-5&#125;/*文字坐标*/, FONT_HERSHEY_PLAIN, 1, Scalar(0, 69, 255), 2);\t\t&#125;\t&#125;&#125;//void main() &#123;//\tstring path = &quot;Resources/shapes.png&quot;;//\tMat img=imread(path);//\t//\t//在检测形状前，对图片预处理：转换为灰度、添加高斯模糊、使用Canny边缘检测器、扩张边缘//\tMat imgGray, imgBlur, imgCanny, imgDil, imgErode;//\tcvtColor(img, imgGray, COLOR_BGR2GRAY);//cvt是convert的缩写，将图像从一种颜色空间转换为另一种颜色空间。//\tGaussianBlur(imgGray, imgBlur,Size(3,3),3,0);//使用高斯滤波器模糊图像。该函数将源图像与指定的高斯核进行卷积,Size(7,7)是核大小,数字越大越模糊//\tCanny(imgBlur, imgCanny, 25, 75);//边缘检测，阈值1，2可调，目的：显示更多的边缘//\tMat kernel = getStructuringElement(MORPH_RECT, Size(3, 3));//创建一个核，增加Size（只能是奇数）会扩张/侵蚀更多//\tdilate(imgCanny, imgDil, kernel);//扩张边缘（增加边缘厚度）//\t//\tgetContours(imgDil,img);//img是在其上绘轮廓的图片////\timshow(&quot;Image&quot;, img);//\t/*imshow(&quot;Image Gray&quot;, imgGray);//\timshow(&quot;Image Blur&quot;, imgBlur);//\timshow(&quot;Image Canny&quot;, imgCanny);//\timshow(&quot;Image Dil&quot;, imgDil);*///\twaitKey(0);//增加延时，0表示无穷//&#125;\nChapter 8//学习检测图像中的面部#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;opencv2/objdetect.hpp&gt;//对象检测头文件#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Images/// &lt;/summary&gt;//void main() &#123;//\tstring path = &quot;Resources/test.png&quot;;//\tMat img=imread(path);//\tCascadeClassifier faceCascade;/*用于对象检测的级联分类器类*///\tfaceCascade.load(&quot;Resources/haarcascade_frontalface_default.xml&quot;);//从文件加载分类器(已经训练好的模型)////\tif (faceCascade.empty()) &#123; cout &lt;&lt; &quot;XML file not loaded&quot; &lt;&lt; endl; &#125;//检测文件是否加载成功////\tvector&lt;Rect&gt; faces;//\tfaceCascade.detectMultiScale(img/*输入*/, faces/*输出*/, 1.1/*比例因子*/, 10/*最小邻居*/);//在输入图像中检测不同大小的对象。检测到的对象将以矩形列表的形式返回。//\t//\tfor (int i = 0; i &lt; faces.size(); i++) &#123;//\t\trectangle(img, faces[i].tl(),faces[i].br(), Scalar(255, 0, 255), 3);//绘制矩形//\t&#125;////\timshow(&quot;Image&quot;, img);//\twaitKey(0);//增加延时，0表示无穷//&#125;\nProject 1#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Project 1/// 使用HSV空间检测颜色、找到轮廓所在位置、取轮廓的位置然后创建一个圆/// &lt;/summary&gt;//Mat img;//vector&lt;vector&lt;int&gt;&gt; newPoints;////vector&lt;vector&lt;int&gt;&gt; myColors&#123; &#123;98,109,54,127,255,255&#125;,//蓝色（hmin smin vmin hmax smax vmax）//\t\t\t\t\t\t\t\t&#123;35,0,0,77,245,255&#125; &#125;;//绿色（hmin smin vmin hmax smax vmax）////vector&lt;Scalar&gt; myColorValues&#123; &#123;255,0,255&#125;,//蓝色//\t\t\t\t\t\t\t\t&#123;0,255,0&#125; &#125;;//绿色//////! 获取轮廓//Point getContours(Mat imgDil) &#123;//imgDil是传入的扩张边缘的图像用来查找轮廓，img是要在其上绘制轮廓的图像//\tvector&lt;vector&lt;Point&gt;&gt; contours;//轮廓检测到的轮廓。每个轮廓线存储为一个点的向量////\tvector&lt;Vec4i&gt; hierarchy;//包含关于映像拓扑的信息  typedef Vec&lt;int, 4&gt; Vec4i;具有4个整数值////\t//在二值图像中查找轮廓。该函数利用该算法从二值图像中提取轮廓//\tfindContours(imgDil, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);//\t//drawContours(img, contours, -1, Scalar(255, 0, 255), 2);//img：要绘制轮廓在什么图片上，contours：要绘制的轮廓，-1定义要绘制的轮廓号（-1表示所有轮廓），Saclar表示轮廓颜色，2表示厚度//\t//\tvector&lt;vector&lt;Point&gt;&gt; conPoly(contours.size());//conploy的数量应小于contours//\tvector&lt;Rect&gt; boundRect(contours.size());////\tPoint myPoint(0, 0);////\t//过滤器：通过轮廓面积来过滤噪声//\tfor (int i = 0; i &lt; contours.size(); i++) &#123;//遍历检测到的轮廓//\t\tint area = contourArea(contours[i]);//\t\t//\t\t//cout &lt;&lt; area &lt;&lt; endl;////\t\tstring objectType;//\t\tif (area &gt; 1000) &#123;//轮廓面积＞1000才绘制//\t\t\t//计算轮廓周长或凹坑长度。该函数计算了曲线长度和封闭的周长。//\t\t\tfloat peri = arcLength(contours[i], true);//计算封闭轮廓周长//\t\t\tapproxPolyDP(contours[i], conPoly[i], 0.02 * peri, true);//以指定的精度近似多边形曲线。第二个参数conPloy[i]存储近似的结果，是输出。////\t\t\tboundRect[i] = boundingRect(conPoly[i]);//计算边界矩形////\t\t\tmyPoint.x = boundRect[i].x + boundRect[i].width / 2;//\t\t\tmyPoint.y = boundRect[i].y;////\t\t\trectangle/*绘制边界矩形*/(img, boundRect[i].tl()/*tl()：topleft矩形左上角坐标*/, boundRect[i].br()/*br()：bottom right矩形右下角坐标*/, Scalar(0, 255, 0), 5);//\t\t\tdrawContours(img, conPoly, i, Scalar(255, 0, 255), 2);//\t\t\t//\t\t&#125;//\t&#125;//\treturn myPoint;//&#125;//////vector&lt;vector&lt;int&gt;&gt; findColor(Mat img) &#123;//\tMat imgHSV;//\tcvtColor(img, imgHSV, COLOR_BGR2HSV);//转换图像到HSV空间，在其中查找颜色更加容易////\tfor (int i = 0; i &lt; myColors.size(); i++)//\t&#123;//\t\tScalar lower(myColors[i][0], myColors[i][1], myColors[i][2]);//\t\tScalar upper(myColors[i][3], myColors[i][4], myColors[i][5]);//\t\tMat mask;//\t\tinRange(imgHSV, lower, upper, mask);//定义颜色下限和上限，因为由于照明和不同的阴影，颜色的值将不完全相同，会是一个值的范围//\t\t//imshow(to_string(i), mask);//\t\tPoint myPoint=getContours(mask);//\t\tif (myPoint.x != 0 &amp;&amp; myPoint.y != 0) &#123;//没检测到东西的时候就不加入新点//\t\t\tnewPoints.push_back(&#123; myPoint.x,myPoint.y,i &#125;);//i为颜色索引//\t\t&#125;//\t&#125;//\treturn newPoints;//\t//&#125;////void drawOnCanvas(vector&lt;vector&lt;int&gt;&gt; newPoints, vector&lt;Scalar&gt; myColorValues) &#123;//\tfor (int i = 0; i &lt; newPoints.size(); i++) &#123;//\t\tcircle(img, Point(newPoints[i][0], newPoints[i][1]),6,myColorValues[newPoints[i][2]],FILLED);//\t&#125;//&#125;////void main() &#123;//\tVideoCapture cap(0);//相机id=0//\t////\twhile (true) &#123;//\t\tcap.read(img);////\t\tnewPoints=findColor(img);//\t\tdrawOnCanvas(newPoints,myColorValues);//\t\timshow(&quot;Image&quot;, img);//\t\twaitKey(1);//增加延时 1ms，以免太慢//\t&#125;////&#125;\nProject 2#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Project 2 – Document Scanner/// 图像预处理：转换为灰度、添加模糊、使用Canny边缘检测器找到边缘（知道纸张在哪里）、基于纸张的坐标提取四个角得到顶视图/// &lt;/summary&gt;//Mat imgOriginal, imgGray, imgBlur, imgCanny, imgDil, imgErode, imgThre, imgWarp, imgCrop;//vector&lt;Point&gt; initialPoints,docPoints;//float w = 420, h = 596;//Mat preProcessing(Mat img) &#123;//\tcvtColor(img, imgGray, COLOR_BGR2GRAY);//cvt是convert的缩写，将图像从一种颜色空间转换为另一种颜色空间。//\tGaussianBlur(imgGray, imgBlur,Size(7,7),5,0);//使用高斯滤波器模糊图像。该函数将源图像与指定的高斯核进行卷积,Size(7,7)是核大小,数字越大越模糊//\tCanny(imgBlur, imgCanny, 25, 75);//边缘检测，阈值1，2可调，目的：显示更多的边缘//\t//\tMat kernel = getStructuringElement(MORPH_RECT, Size(3, 3));//创建一个核，增加Size（只能是奇数）会扩张/侵蚀更多//\tdilate(imgCanny, imgDil, kernel);//扩张边缘（增加边缘厚度）//\t//erode(imgDil, imgErode, kernel);//侵蚀边缘（减小边缘厚度）//\treturn imgDil;//&#125;////vector&lt;Point&gt;/*返回纸张的4个角点*/ getContours(Mat imgDil) &#123;//imgDil是传入的扩张边缘的图像用来查找轮廓，img是要在其上绘制轮廓的图像//\tvector&lt;vector&lt;Point&gt;&gt; contours;//轮廓检测到的轮廓。每个轮廓线存储为一个点的向量////\tvector&lt;Vec4i&gt; hierarchy;//包含关于映像拓扑的信息  typedef Vec&lt;int, 4&gt; Vec4i;具有4个整数值////\t//在二值图像中查找轮廓。该函数利用该算法从二值图像中提取轮廓//\tfindContours(imgDil, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);//\t//drawContours(img, contours, -1, Scalar(255, 0, 255), 2);//img：要绘制轮廓在什么图片上，contours：要绘制的轮廓，-1定义要绘制的轮廓号（-1表示所有轮廓），Saclar表示轮廓颜色，2表示厚度//\t//\tvector&lt;vector&lt;Point&gt;&gt; conPoly(contours.size());//conploy的数量应小于contours//\tvector&lt;Rect&gt; boundRect(contours.size());////\tvector&lt;Point&gt; biggest;//\tint maxArea = 0;////\t//过滤器：通过轮廓面积来过滤噪声//\tfor (int i = 0; i &lt; contours.size(); i++) &#123;//遍历检测到的轮廓//\t\tint area = contourArea(contours[i]);//\t\t//\t\t//cout &lt;&lt; area &lt;&lt; endl;////\t\tstring objectType;//\t\tif (area &gt; 1000) &#123;//轮廓面积＞1000才绘制//\t\t\t//计算轮廓周长或凹坑长度。该函数计算了曲线长度和封闭的周长。//\t\t\tfloat peri = arcLength(contours[i], true);//计算封闭轮廓周长//\t\t\tapproxPolyDP(contours[i], conPoly[i], 0.02 * peri, true);//以指定的精度近似多边形曲线。第二个参数conPloy[i]存储近似的结果，是输出。//\t\t\t//\t\t\tif (area &gt; maxArea&amp;&amp;conPoly[i].size()==4) &#123;//\t\t\t\tbiggest = &#123; conPoly[i][0],conPoly[i][1],conPoly[i][2],conPoly[i][3] &#125;;//\t\t\t\tmaxArea = area;//\t\t\t&#125;////\t\t\t//rectangle/*绘制边界矩形*/(imgOriginal, boundRect[i].tl()/*tl()：topleft矩形左上角坐标*/, boundRect[i].br()/*br()：bottom right矩形右下角坐标*/, Scalar(0, 255, 0), 5);//\t\t\t//drawContours(imgOriginal, conPoly, i, Scalar(255, 0, 255), 2);//\t\t\t//\t\t&#125;//\t&#125;//\treturn biggest;//&#125;////void drawPoints(vector&lt;Point&gt; points, Scalar color) &#123;//\tfor (int i = 0; i &lt; points.size(); i++) &#123;//\t\tcircle(imgOriginal, points[i], 10, color, FILLED);//\t\tputText(imgOriginal, to_string(i), points[i], FONT_HERSHEY_PLAIN, 4, color, 4);//\t&#125;//&#125;////vector&lt;Point&gt; reorder(vector&lt;Point&gt; points) &#123;//标记点的顺序会变，要确定一个顺序 0 1//\t\t\t\t\t\t\t\t\t\t\t//\t\t\t\t\t\t\t\t 2 3//\tvector&lt;Point&gt; newPoints;//\tvector&lt;int&gt;  sumPoints, subPoints;//\tfor (int i = 0; i &lt; 4; i++) &#123;//\t\tsumPoints.push_back(points[i].x + points[i].y);//\t\tsubPoints.push_back(points[i].x - points[i].y);//\t&#125;//\t//\tnewPoints.push_back(points[min_element/* find smallest element*/(sumPoints.begin(), sumPoints.end())-sumPoints.begin()]);//\tnewPoints.push_back(points[max_element/* find largest element*/(subPoints.begin(), subPoints.end()) - subPoints.begin()]);//\tnewPoints.push_back(points[min_element/* find smallest element*/(subPoints.begin(), subPoints.end()) - subPoints.begin()]);//\tnewPoints.push_back(points[max_element/* find largest element*/(sumPoints.begin(), sumPoints.end()) - sumPoints.begin()]);////\treturn newPoints;//&#125;////Mat getWarp(Mat img,vector&lt;Point&gt; points,float w,float h) &#123;//\tPoint2f src[4] = &#123; points[0],points[1],points[2],points[3] &#125;;//Point2f表示浮点数//\tPoint2f dst[4] = &#123; &#123;0.0f,0.0f&#125;,&#123;w,0.0f&#125;,&#123;0.0f,h&#125;,&#123;w,h&#125; &#125;;//Point2f表示浮点数////\tMat matrix = getPerspectiveTransform(src, dst);//\twarpPerspective(img, imgWarp, matrix, Point(w,h));////\treturn imgWarp;//&#125;////void main() &#123;//\tstring path = &quot;Resources/paper.jpg&quot;;//\timgOriginal=imread(path);////\t//resize(imgOriginal/*source*/, imgOriginal/*destination*/, Size()/*不定义尺寸*/, 0.5/*定义比例*/, 0.5/*定义比例*/);////\t//预处理//\timgThre = preProcessing(imgOriginal);//\t//获得轮廓--获得最大矩形//\tinitialPoints=getContours(imgThre);//\t//drawPoints(initialPoints, Scalar(0, 0, 255));//\tdocPoints = reorder(initialPoints);//\t//drawPoints(docPoints, Scalar(0, 255, 0));//\t//扭曲//\timgWarp = getWarp(imgOriginal, docPoints, w, h);//\t//裁剪多余的边--通过创建一个矩形//\tint cropValue = 5;//\tRect roi(cropValue/*每条边要减去的像素*/, cropValue, w - 2 * cropValue/*宽度*/, h - 2 * cropValue/*高度*/);//\timgCrop = imgWarp(roi);//\timshow(&quot;Image&quot;, imgOriginal);//\timshow(&quot;Image Dilation&quot;, imgThre);//\timshow(&quot;Image Warp&quot;, imgWarp);//\timshow(&quot;Image Crop&quot;, imgCrop);//\twaitKey(0);//&#125;\nProject 3//学习如何检测车牌和如何裁剪并保存这些区域#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;opencv2/objdetect.hpp&gt;//对象检测头文件#include&lt;iostream&gt;using namespace std;using namespace cv;/// &lt;summary&gt;/// Project 3 – License Plate(车牌) Detector/// &lt;/summary&gt;void main() &#123;\t\tVideoCapture cap(0);//相机id=0\tMat img;\t//加载模型\tCascadeClassifier plateCascade;/*用于对象检测的级联分类器类*/\tplateCascade.load(&quot;Resources/haarcascade_russian_plate_number.xml&quot;);//从文件加载分类器(已经训练好的模型)\tif (plateCascade.empty()) &#123; cout &lt;&lt; &quot;XML file not loaded&quot; &lt;&lt; endl; &#125;//检测文件是否加载成功\tvector&lt;Rect&gt; plates;\twhile (true) &#123;\t\tcap.read(img);\t\t//可以更改比例因子和最小邻居来调整检测成功率\t\tplateCascade.detectMultiScale(img/*输入*/, plates/*输出*/, 1.1/*比例因子*/, 10/*最小邻居*/);//在输入图像中检测不同大小的对象。检测到的对象将以矩形列表的形式返回。\t\t\t\t\t\tfor (int i = 0; i &lt; plates.size(); i++) &#123;\t\t\tMat imgCrop = img(plates[i]);//plates是矩形列表，plates[i]是矩形\t\t\t//imshow(to_string(i), imgCrop);\t\t\timwrite(&quot;Resources/Plates/&quot; + to_string(i) + &quot;.png&quot;, imgCrop);\t\t\trectangle(img, plates[i].tl(), plates[i].br(), Scalar(255, 0, 255), 3);//绘制矩形\t\t&#125;\t\timshow(&quot;Image&quot;, img);\t\twaitKey(1);//增加延时，0表示无穷\t&#125;&#125;\nColorpicker#include&lt;opencv2/imgcodecs.hpp&gt;#include&lt;opencv2/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;#include&lt;iostream&gt;using namespace std;using namespace cv;//int hmin = 0, smin = 0, vmin = 0;//int hmax = 179, smax = 255, vmax = 255;//如何确定这6个值，每次都更改所有这些再次运行很痛苦 --&gt;创建跟踪栏（使我们可以实时更改这些值）//Mat img;//Mat imgHSV, mask, imgColor;//void main() &#123;//\tVideoCapture cap(0);//相机id=0//\t//\t//\tnamedWindow(&quot;Trackbars&quot;, (640, 200));//创建窗口，(640,200)是尺寸//\t//运行时，把3个min的都移到最小值，把3个max的都移到最大值，然后移动使其保持为白色//\tcreateTrackbar(&quot;Hue Min&quot;, &quot;Trackbars&quot;, &amp;hmin, 179);//对于hue色相饱和度最大180,对于另外两个色相饱和度最大255//\tcreateTrackbar(&quot;Hue Max&quot;, &quot;Trackbars&quot;, &amp;hmax, 179);//\tcreateTrackbar(&quot;Sat Min&quot;, &quot;Trackbars&quot;, &amp;smin, 255);//\tcreateTrackbar(&quot;Sat Max&quot;, &quot;Trackbars&quot;, &amp;smax, 255);//\tcreateTrackbar(&quot;Val Min&quot;, &quot;Trackbars&quot;, &amp;vmin, 255);//\tcreateTrackbar(&quot;Val Max&quot;, &quot;Trackbars&quot;, &amp;vmax, 255);//\t//\twhile (true) &#123;//\t\t//检查数组元素是否位于其他两个数组的元素之间。//\t\t//imgHSV为输入图像，mask为输出图像//\t\tcap.read(img);//\t\tcvtColor(img, imgHSV, COLOR_BGR2HSV);//转换图像到HSV空间，在其中查找颜色更加容易//\t\tScalar lower(hmin, smin, vmin);//\t\tScalar upper(hmax, smax, vmax);//\t\tinRange(imgHSV, lower, upper, mask);//定义颜色下限和上限，因为由于照明和不同的阴影，颜色的值将不完全相同，会是一个值的范围//\t\tcout &lt;&lt; hmin &lt;&lt; &quot;,&quot; &lt;&lt; smin &lt;&lt; &quot;,&quot; &lt;&lt; vmin &lt;&lt; &quot;,&quot; &lt;&lt; hmax &lt;&lt; &quot;,&quot; &lt;&lt; smax &lt;&lt; &quot;,&quot; &lt;&lt; vmax &lt;&lt; endl;//\t\timshow(&quot;Image&quot;, img);//\t\timshow(&quot;Image HSV&quot;, imgHSV);//\t\timshow(&quot;Image mask&quot;, mask);//\t\twaitKey(1);//增加延时//\t&#125;//&#125;\n","categories":["HDU's Learning"],"tags":["OpenCV"]},{"title":"OpenCV (视频读写&&背景消去&&对象跟踪)","url":"/2021/07/28/OPENCV1-2/","content":"这是Eva今年暑假学的~\n\nChapter 1 视频读写#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main(/*int argc, char** argv*/)&#123; //括号里应该不重要吧\tVideoCapture capture;\tcapture.open(&quot;D:/.../&quot;);         //要打开的文件的路径\tif(!capture.isOpened())&#123;         //如果打开失败了\t\tcout &lt;&lt; &quot;could not load&quot; &lt;&lt; endl;\t\treturn -1; \t&#125;\t\tdouble fps = capture.get(CV_CAP_PROP_FPS);\tVedioWriter writer(&quot;...&quot;, -1, fps, Size(640, 480), true);\t//Size size = Size(capture.get(CV_CAP_PROP_FRAME_WIDTH), capture.get(CV_CAP_PROP_FRAME_HEIGHT));\t\tMat frame, gray, binary;\tnamedWindow(&quot;Video-demo&quot;, CV_WINDOW_AUTOSIZE);\twhile(capture.read(frame))&#123;\t\t//imshow(&quot;Video-demo&quot;, frame);\t\t//cvtColor(frame, gray, COLOR_BGR2GRAY);        //转换成灰度图\t\t//imshow(&quot;Video-demo&quot;, gray);\t\t//threshold(gray, binary, 0, 255, THRESH_BINARY|THRESH_OTSU);      //二值化 \t\t//imshow(&quot;Video-demo&quot;, binary);\t\tbitwise_not(frame, frame);         //bitwise_xor,bitwise_and\t\timshow(&quot;Video-demo&quot;, frame);\t\t//writer.write(frame);\t\tchar c = waitKey(100);\t\tif(c == 27)&#123;\t\t\tbreak;\t\t&#125;\t&#125;\t\twaitKey(0);\treturn 0;&#125;\n相关类和函数Capture 类//构造函数：filename – 打开的视频文件名。device – 打开的视频捕获设备id ，如果只有一个摄像头可以填0，表示打开默认的摄像头。 VideoCapture::VideoCapture();VideoCapture::VideoCapture(const string&amp; filename);VideoCapture::VideoCapture(int device);//open 打开一个视频文件或者打开一个捕获视频的设备bool VideoCapture::open(const string&amp; filename);bool VideoCapture::open(int device);//先实例化再初始化VideoCapture capture;capture.open(&quot;dog.avi&quot;);//在实例化的同时进行初始化VideoCapture(&quot;dog.avi&quot;);//isopenbool VideoCapture::isOpened();//release 关闭视频文件或者摄像头void VideoCapture::release();//grab 抓取下一个帧，假如调用成功返回truebool VideoCapture::grab();//retrieve 解码并且返回刚刚抓取的视频帧bool VideoCapture::retrieve(Mat&amp; image, int channel=0);//read 该函数结合VideoCapture::grab()和VideoCapture::retrieve()其中之一被调用，用于捕获、解码和返回下一个视频帧这是一个最方便的函数对于读取视频文件或者捕获数据从解码和返回刚刚捕获的帧VideoCapture&amp; VideoCapture::operator&gt;&gt;(Mat&amp; image);bool VideoCapture::read(Mat&amp; image);// 方法一 capture.read(frame); // 方法二 capture.grab(); // 方法三capture.retrieve(frame); // 方法四capture &gt;&gt; frame;//get 帧率、总帧数、尺寸、格式等，VideoCapture的get方法可以获取这些属性double VideoCapture::get(int propId);//参数是属性的ID//set 设置属性 (属性ID，要设置的值)bool VideoCapture::set(int propertyId, double value);\n#include &lt;iostream&gt; #include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt; int main(int argc,char* argv[])&#123;    cv::VideoCapture capture(argv[1]);    if(!capture.isOpened())     &#123;        std::cout&lt;&lt;&quot;video not open.&quot;&lt;&lt;std::endl;        return 1;    &#125;    //获取当前视频帧率    double rate = capture.get(CV_CAP_PROP_FPS);    //当前视频帧    cv::Mat frame;    //每一帧之间的延时    //与视频的帧率相对应    int delay = 1000/rate;    bool stop(false);    while(!stop)    &#123;        if(!capture.read(frame))        &#123;            std::cout&lt;&lt;&quot;no video frame&quot;&lt;&lt;std::endl;            break;         &#125;         //此处为添加对视频的每一帧的操作方法        int frame_num = capture.get(CV_CAP_PROP_POS_FRAMES);        std::cout&lt;&lt;&quot;Frame Num : &quot;&lt;&lt;frame_num&lt;&lt;std::endl;        if(frame_num==20)        &#123;            capture.set(CV_CAP_PROP_POS_FRAMES,10);        &#125;         cv::imshow(&quot;video&quot;,frame);        //引入延时        //也可通过按键停止        if(cv::waitKey(delay)&gt;0)        stop = true;    &#125;      //关闭视频，手动调用析构函数（非必须）    capture.release();    return 0;&#125;\nVideoWriter​ 类//三种构造函数VideoWriter::VideoWriter()VideoWriter::VideoWriter(const String &amp;filename, int fourcc, double fps, Size frameSize, bool isColor=true)VideoWriter::VideoWriter(const String &amp;filename, int apiPreference, int fourcc, double fps, Size frameSize, bool isColor=true)\nfilename​ : 输出视频文件的路径名称\nfourcc​ : 字符类型的编码，表示用于编码视频文件的编码器。其中\n\nVideoWriter::fourcc(&#39;P’,’I’,’M’,’1’) 表示 MPEG-1 编码文件扩展名为 .avi​ ; \nVideoWriter::fourcc(&#39;X&#39;,&#39;V&#39;,&#39;I&#39;,&#39;D&#39;) 表示 MPEG-4 编码文件扩展名为 .avi ; \nVideoWriter::fourcc(&#39;X&#39;,’2&#39;,&#39;6&#39;,&#39;4&#39;)​ 表示 MPEG-4​ 编码文件扩展名为 .mp4​ ;\nVideoWriter::fourcc(&#39;I&#39;,’4&#39;,&#39;2&#39;,&#39;0&#39;) 表示 YUV 编码，文件扩展名为.avi ;\nVideoWriter::fourcc(&#39;M&#39;,’P&#39;,&#39;4&#39;,&#39;V&#39;) 表示旧的 MPEG-4 编码，文件扩展名为 .avi ;\nVideoWriter::fourcc(&#39;T&#39;,’H&#39;,&#39;E&#39;,&#39;O&#39;) 表示使用 ogg vorbis ，文件扩展名为 .ogv ;\nVideoWriter::fourcc(&#39;F&#39;,&#39;L&#39;,&#39;V&#39;,&#39;1&#39;) 表示 flash video ,文件扩展名为 .flv ;\n\nfps​ : 表示帧率\nframeSize​ : 表示每一帧图像的大小\nisColor : 灰度图像或者是彩色图像（仅仅在 windows 上支持）\napiPreference ： 使用指定的 API，例如可以使用 cv::CAP_FFMPEG 或者 cv::CAP_GSTREAMER​ 等。\n//常用函数VideoWriter::isOpened()    VideoWriter::getBackednName()    VideoWriter::open(const String &amp;filename, int fourcc, double fps, Size frameSize, bool isColor=true);VideoWriter::open(const String &amp;filename,int apiPreference,int fourcc,double fps,Size frameSize,bool isColor=true);VideoWriter::release()    VideoWriter::get(int propId);VideoWriter::set(int propId,double value);\nSize 类template&lt;typename _Tp&gt; class Size_&#123;public:    typedef _Tp value_type;    //! various constructors    Size_();    Size_(_Tp _width, _Tp _height);    Size_(const Size_&amp; sz);    Size_(const CvSize&amp; sz);    Size_(const CvSize2D32f&amp; sz);    Size_(const Point_&lt;_Tp&gt;&amp; pt);    Size_&amp; operator = (const Size_&amp; sz);    //! the area (width*height)    _Tp area() const;    //! conversion of another data type.    template&lt;typename _Tp2&gt; operator Size_&lt;_Tp2&gt;() const;    //! conversion to the old-style OpenCV types    operator CvSize() const;    operator CvSize2D32f() const;    _Tp width, height; // the width and the height&#125;;\ncvtColor_CV_8U_ 图像 其通道值范围为0到255\n_CV_16U_ 时其值通道值范围为0到65535\n_CV_32F_ 时，其通道值范围为0到1\n//src：为原图片 code：需要进行色彩空间转换的结果void cv::cvtColor (InputArray src, OutputArray dst, int code, int dstCn = 0)\nthreshold去掉噪，例如过滤很小或很大像素值的图像点\ndouble threshold( InputArray src, OutputArray dst, double thresh, double maxval, int type )//src 源图像Mat对象//dst 目标图像Mat对象//thresh 设定的阈值//maxval 是当灰度值大于（或小于）阈值时将该灰度值赋成的值//type 二值化的方式\n二值化的方式, 常用的有如下5种\nCV_THRESH_BINARY      =0,  /**大于阈值的部分被置为255，小于部分被置为0 */CV_THRESH_BINARY_INV  =1,  /**大于阈值部分被置为0，小于部分被置为255    */CV_THRESH_TRUNC       =2,  /**大于阈值部分被置为threshold，小于部分保持原样   */CV_THRESH_TOZERO      =3,  /**小于阈值部分被置为0，大于部分保持不变*/CV_THRESH_TOZERO_INV  =4,  /**大于阈值部分被置为0，小于部分保持不变 */\n#include&lt;opencv2/opencv.hpp&gt;using namespace cv;int main() &#123;    Mat src = imread(&quot;C:/Users/Administrator/Desktop/txyzm.png&quot;);//引入源图像    if (src.empty()) &#123;        return -1;    &#125;    Mat graySrc,dst;    cvtColor(src, graySrc,CV_BGR2GRAY);//转换为灰度图像    threshold(graySrc, dst, 170, 255, CV_THRESH_BINARY);//图像二值化    imshow(&quot;dst&quot;, dst);//展示目标图像    waitKey(0);    return 0;&#125;\nbitwise//bitwise_not 将二指图片的效果反转既黑色变白色，白色变黑色bitwise_not(InputArray src, OutputArray dst, InputArray mask = noArray());//bitwise_xor 对两个图像进行”异“处理//bitwise_or 计算每个位操作分离的两个数组或一个数\nnamedWindowvoid nameWindow(const string&amp; winname,int flags = WINDOW_AUTOSIZE) ;\n\nQt后端支持标志：\n\nWINDOW_NORMAL 或 WINDOW_AUTOSIZEWINDOW_NORMAL 使您可以调整大小窗口，而 WINDOW_AUTOSIZE 自动调整窗口大小以适应显示图像（参见 imshow ），您无法手动更改窗口大小。\nWINDOW_FREERATIO 或 WINDOW_KEEPRATIOWINDOW_FREERATIO 调整图像不考虑其比例，而 WINDOW_KEEPRATIO 保持图像比例。\nWINDOW_GUI_NORMAL 或 WINDOW_GUI_EXPANDEDWINDOW_GUI_NORMAL 是绘制窗口的旧方法没有状态栏和工具栏，而                   WINDOW_GUI_EXPANDED 是一个新的增强 GUI 。默认情况下，flags == WINDOW_AUTOSIZE | WINDOW_KEEPRATIO |WINDOW_GUI_EXPANDED \n\n\n\nChapter 2 背景消去建模（BSM）：背景不常变化的图像分割 （GMM — 高斯混合模型）\nPtr&lt;BackgroundSubtractorMOG2&gt; bgsubtractor = createBackgroundSubtractorMOG2();bgsubtractor-&gt;setHistory(20);bgsubtractor-&gt;setVarThreshold(100);bgsubtractor-&gt;setDetectShadows(true);bgsubtractor-&gt;setBackgroundRatio(4);bgsubtractor-&gt;setNMixtures(5);bgsubtractor-&gt;setShadowThreshold(40);bgsubtractor-&gt;setVarInit(15);bgsubtractor-&gt;setVarMax(20);bgsubtractor-&gt;setVarMin(4);bgsubtractor-&gt;setVarThresholdGen(100);\nHistory：用于训练背景的帧数，history可以用于计算当前的learning rate ，history越大，learning rate越低，背景更新越缓慢；\nVarThreshold：方差阈值，主要用于判断前景还是背景，值越大，灵敏度越低\nDetectShadows：是否检测有影子，开启后会增加算法复杂度\nNMixtures：高斯模型个数，默认5个，最多8个，模型数越多，耗时越长\nBackgroundRatio：高斯背景模型权重和阈值，nmixtures个模型按权重排序后，只取模型权重累加值大于backgroundRatio的前几个作为背景模型\nVarInit：新建高斯模型的方差初始值，默认15\nVarMax：背景更新时，用于限制高斯模型方差的最大值，默认20\nVarMin：背景更新时，用于限制高斯模型方差的最小值，默认4\nVarThresholdGen：方差阈值，用于已经存在的匹配的模型，如果不存在则新建一个\n原文链接：https://blog.csdn.net/holecloud/article/details/80139297\n#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123;\tVideoCapture capture;\tcapture.open(&quot;...&quot;);\tif(!capture.isOpened())&#123;         //如果打开失败了\t\tcout &lt;&lt; &quot;could not load&quot; &lt;&lt; endl;\t\treturn -1; \t&#125;\t\tMat frame;\tMat bsmaskMOG2;\tnamedWindow(&quot;input video&quot;, CV_WINDOW_AUTOSIZE);\tnamedWindow(&quot;MOG2&quot;, CV_WINDOW_AUTOSIZE);\tPtr&lt;BackgroudSubtractor&gt; pMOG2 = createBackgroundSubtractorMOG2();  //选择API\twhile(capture.read(frame))&#123;\t\timshow(&quot;inpupt video&quot;, frame);\t\tpMOG2 -&gt; apply(frame, bsmaskMOG2);\t\timshow(&quot;MOG2&quot;, bsmaskMOG2);\t\tchar c = waitKey(100);\t\tif(c == 27)&#123;\t\t\tbreak;\t\t&#125;\t&#125;\t\tcapture.release();\twaitKey(0);\treturn 0;&#125;\n机器学习（KNN — K​ 个最近邻）\n#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123;\tVideoCapture capture;\tcapture.open(&quot;...&quot;);\tif(!capture.isOpened())&#123;         //如果打开失败了\t\tcout &lt;&lt; &quot;could not load&quot; &lt;&lt; endl;\t\treturn -1; \t&#125;\t\tMat frame;\tMat bsmaskKNN;\tnamedWindow(&quot;input video&quot;, CV_WINDOW_AUTOSIZE);\tnamedWindow(&quot;KNN&quot;, CV_WINDOW_AUTOSIZE);\tPtr&lt;BackgroudSubtractor&gt; pKNN = createBackgroundSubtractorKNN();\twhile(capture.read(frame))&#123;\t\timshow(&quot;inpupt video&quot;, frame);\t\tpKNN -&gt; apply(frame, bsmaskKNN);\t\timshow(&quot;KNN&quot;, bsmaskKNN);\t\tchar c = waitKey(100);\t\tif(c == 27)&#123;\t\t\tbreak;\t\t&#125;\t&#125;\t\tcapture.release();\twaitKey(0);\treturn 0;&#125;\n开操作去噪声\n#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123;\tVideoCapture capture;\tcapture.open(&quot;...&quot;);\tif(!capture.isOpened())&#123;         //如果打开失败了\t\tcout &lt;&lt; &quot;could not load&quot; &lt;&lt; endl;\t\treturn -1; \t&#125;\t\tMat frame;\tMat bsmaskMOG2;\tnamedWindow(&quot;input video&quot;, CV_WINDOW_AUTOSIZE);\tnamedWindow(&quot;MOG2&quot;, CV_WINDOW_AUTOSIZE);\t\tMat kernel = getStructuringELement(MORPH_RECT, Size(3, 3), Point(-1, -1));\t\tPtr&lt;BackgroudSubtractor&gt; pMOG2 = createBackgroundSubtractorMOG2();  //选择API\twhile(capture.read(frame))&#123;\t\timshow(&quot;inpupt video&quot;, frame);\t\tpMOG2 -&gt; apply(frame, bsmaskMOG2);\t\tmorphologyEx(bsmaskMOG2, bsmaskMOG2, MORPH_OPEN, kernel, Point(-1, -1));\t\timshow(&quot;MOG2&quot;, bsmaskMOG2);\t\tchar c = waitKey(50);\t\tif(c == 27)&#123;\t\t\tbreak;\t\t&#125;\t&#125;\t\tcapture.release();\twaitKey(0);\treturn 0;&#125;\n相关类和函数BackgroundSubtractor\nBackgroundSubtractor （父类）   -  BackgroundSubtractorMOG2    -  BackgroundSubtractorKNN\nChapter 3 对象检测与跟踪：基于颜色inRange过滤利用颜色进行过滤\n形态学操作提取开操作 去噪声 膨胀 \n轮廓查找外接矩形获取位置标定#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;Rect roi;void processFrame(Mat &amp;mask, Rect &amp;rect);int main()&#123;\tVideoCapture capture;\tcapture.open(&quot;...&quot;);\tif(!capture.isOpened())&#123;         //如果打开失败了\t\tcout &lt;&lt; &quot;could not load&quot; &lt;&lt; endl;\t\treturn -1; \t&#125;\t\tMat frame, mask;\tMat kernel1 = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1));\tMat kernel2 = getStructuringElement(MORPH_RECT, Size(5, 5), Point(-1, -1));\t\tnamedWindow(&quot;input video&quot;, CV_WINDOW_AUTOSIZE);\tnamedWindow(&quot;track mask&quot;, CV_WINDOW_AUTOSIZE);\twhile(capture.read(frame))&#123;\t\tinRange(frame, Scalar(0, 0, 127), Scalar(120, 255, 120), mask); //过滤 \t\tmorphologyEx(mask, mask, MORPH_OPEN, kernel1, Point(-1, -1), 1); //开操作\t\timshow(&quot;track mask&quot;, mask);\t\tdilate(mask, mask, kernel2, Point(-1, -1), 4); //膨胀\t\timshow(&quot;dilate mask&quot;, mask);\t\t\t\tprocessFrame(mask, roi); //找轮廓并标定\t\t\t\trectangle(frame, roi, Scalar(0, 0, 255), 3, 8, 0);\t\t\t\timshow(&quot;input video&quot;, frame);\t\tchar c = waitKey(50);\t\tif(c == 27)&#123;\t\t\tbreak;\t\t&#125;\t&#125;\t\tcapture.release();\twaitKey(0);\treturn 0;&#125;void processFrame(Mat &amp;mask, Rect &amp;rect)&#123; //查找轮廓 \tvector&lt;vector&lt;Point&gt; &gt; contours;\tvector&lt;Vec4i&gt; hireachy;\tfindContours(mask, contours, hireachy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point(0, 0));\tif(contours.size() &gt; 0)&#123;\t\tdouble maxArea = 0.0;\t\tfor(size_t t = 0; t &lt; contours.size(); ++t)&#123;\t\t\tdouble area = contourArea(contours[static_cast&lt;int&gt;(t)]);\t\t\tif(area &gt; maxArea)&#123;\t\t\t\tmaxArea = area;\t\t\t\trect = boundingRect(contours[static_cast&lt;int&gt;(t)]);\t\t\t&#125;\t\t&#125;\t&#125;else&#123;\t\trect.x = rect.y = rect.width = rect.height = 0;\t&#125;&#125;\n相关类和函数inRange()\n   OpenCV中的inRange()函数可实现二值化功能（这点类似threshold()函数），更关键的是可以同时针对多通道进行操作，使用起来非常方便！主要是将在两个阈值内的像素值设置为白色（255），而不在阈值区间内的像素值设置为黑色（0），该功能类似于之间所讲的双阈值化操作。\n  void inRange(InputArray src, InputArray lowerb, InputArray upperb, OutputArray dst);//src 输入要处理的图像，可以为单通道或多通道//lowerb 包含下边界的数组或标量//upperb 包含上边界数组或标量//dst 输出图像，与输入图像src 尺寸相同且为CV_8U 类型\nChecks if array elements lie between the elements of two other arrays.即检查数组元素是否在另外两个数组元素值之间。这里的数组通常也就是矩阵Mat或向量。\n\n该函数输出的dst是一幅二值化之后的图像。\n\nmorphologyEx\n形态学变化函数\n(4条消息) opencv 形态学变换 morphologyEx函数_keen_zuxwang的博客-CSDN博客\nfindContours\n检测物体轮廓\nfindContours(InputOutputArray image, OutputArrayOfArrays contours,  OutputArray hierarchy, int mode, int method, Point offset = Point());//image 单通道图像矩阵 灰度图or二值图像（Canny、拉普拉斯等边缘检测算子）//contours 向量vector&lt;vector&lt;Point&gt;&gt; contours//hierarchy 向量vector&lt;Vec4i&gt; hierarchy   vec4i:typedef Vec&lt;int, 4&gt; Vec4i; //mode 轮廓检索模式//method 定义轮廓的近似方法//Point偏移量 所有的轮廓信息相对于原始图像对应点的偏移量 相当于在每一个检测出的轮廓点上加上该偏移量 并且Point还可以是负值!\n轮廓检索模式\nCV_RETR_EXTERNAL //只检测最外围轮廓CV_RETR_LIST //检测所有的轮廓 轮廓不建立等级关系 即不存在父轮廓或内嵌轮廓CV_RETR_CCOMP //检测所有的轮廓 建立两个等级关系CV_RETR_TREE //检测所有轮廓 所有轮廓建立一个等级树结构\n定义轮廓的近似方法\nCV_CHAIN_APPROX_NONE //保存物体边界上所有连续的轮廓点到contours向量内CV_CHAIN_APPROX_SIMPLE //仅保存轮廓的拐点信息 把所有轮廓拐点处的点保存入contours  CV_CHAIN_APPROX_TC89_L1 //使用teh-Chinl chain 近似算法CV_CHAIN_APPROX_TC89_KCOS //使用teh-Chinl chain 近似算法\ncontourArea\n计算轮廓面积\ndouble contourArea(InputArray contour, bool oriented = false);//contour 输入的二维点集（轮廓顶点）可以是 vector 或 Mat 类型//riented，面向区域标识符 有默认值 false 若为 true 该函数返回一个带符号的面积值 正负取决于轮廓的方向（顺时针还是逆时针） 若为 false 表示以绝对值返回\narcLength\n计算封闭轮廓周长\ndouble arcLength(InputArray curve, bool closed);//contour 输入的二维点集（轮廓顶点）可以是 vector 或 Mat 类型//closed 用于指示曲线是否封闭\nboundingRect\n计算轮廓的垂直边界最小矩形，矩形是与图像上下边界平行的\nRect boundingRect(InputArray points);//points 输入的二维点集（轮廓顶点）可以是 vector 或 Mat 类型","categories":["HDU's Learning"],"tags":["OpenCV"]}]